{"posts":[{"title":"Yolo11äº¤é€šæ ‡å¿—æ£€æµ‹ç³»ç»Ÿ","text":"å…³äºyoloæ¨¡å‹çš„äº¤é€šæ ‡å¿—æ£€æµ‹é¡¹ç›® YOLOv11äº¤é€šæ ‡å¿—æ£€æµ‹é¡¹ç›®ä¸€ã€é¡¹ç›®æ¦‚è¿°1.1 é¡¹ç›®ç®€ä»‹æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäºYOLOv11çš„æ™ºèƒ½äº¤é€šæ ‡å¿—æ£€æµ‹ç³»ç»Ÿï¼Œæ”¯æŒå®æ—¶æ£€æµ‹ã€å›¾ç‰‡æ£€æµ‹å’Œè§†é¢‘æ£€æµ‹ä¸‰ç§æ¨¡å¼ã€‚é¡¹ç›®é‡‡ç”¨PyQt5æ„å»ºç°ä»£åŒ–GUIç•Œé¢ï¼Œé›†æˆäº†åŸå§‹è®­ç»ƒè„šæœ¬å’Œä¼˜åŒ–è®­ç»ƒè„šæœ¬ï¼Œä¸“é—¨é’ˆå¯¹å°ç›®æ ‡æ£€æµ‹è¿›è¡Œäº†æ·±åº¦ä¼˜åŒ–ã€‚ 1.2 ä¸»è¦ç‰¹æ€§ å¤šæ¨¡å¼æ£€æµ‹ï¼šæ”¯æŒæ‘„åƒå¤´å®æ—¶æ£€æµ‹ã€å›¾ç‰‡æ‰¹é‡æ£€æµ‹ã€è§†é¢‘æµæ£€æµ‹ ç°ä»£åŒ–UIï¼šé‡‡ç”¨æ¸å˜è‰²å½©å’Œåœ†è§’è®¾è®¡ï¼Œæä¾›è‰¯å¥½çš„ç”¨æˆ·ä½“éªŒ æ™ºèƒ½ä¼˜åŒ–ï¼šé›†æˆæ³¨æ„åŠ›æœºåˆ¶å’Œç‰¹å¾èåˆï¼Œæå‡å°ç›®æ ‡æ£€æµ‹èƒ½åŠ› å®Œæ•´æµç¨‹ï¼šåŒ…å«æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€éªŒè¯ã€å¯¼å‡ºå’Œéƒ¨ç½²çš„å®Œæ•´æµç¨‹ ç»Ÿè®¡åˆ†æï¼šæä¾›è¯¦ç»†çš„æ£€æµ‹ç»Ÿè®¡å’Œå†å²è®°å½•åŠŸèƒ½ 1.3 æŠ€æœ¯æ ˆ æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼šPyTorch + Ultralytics YOLOv11 GUIæ¡†æ¶ï¼šPyQt5 è®¡ç®—æœºè§†è§‰ï¼šOpenCV æ•°æ®å¤„ç†ï¼šNumPy, PIL é…ç½®ç®¡ç†ï¼šYAML, JSON äºŒã€YOLOè®­ç»ƒå…¨æµç¨‹è¯¦è§£2.1 æ•°æ®å‡†å¤‡é˜¶æ®µ2.1.1 æ•°æ®é›†ç»“æ„data/ â”œâ”€â”€ train/ â”‚ â”œâ”€â”€ images/ # è®­ç»ƒå›¾ç‰‡ (16,356å¼ ) â”‚ â””â”€â”€ labels/ # è®­ç»ƒæ ‡ç­¾ (16,356ä¸ª) â”œâ”€â”€ test/ â”‚ â”œâ”€â”€ images/ # æµ‹è¯•å›¾ç‰‡ (1,500å¼ ) â”‚ â””â”€â”€ labels/ # æµ‹è¯•æ ‡ç­¾ (1,500ä¸ª) â””â”€â”€ dataset.yaml # æ•°æ®é›†é…ç½®æ–‡ä»¶ 2.1.2 æ ‡ç­¾æ ¼å¼é‡‡ç”¨YOLOæ ¼å¼çš„æ ‡ç­¾æ–‡ä»¶ï¼Œæ¯ä¸ªæ ‡ç­¾æ–‡ä»¶åŒ…å«ï¼š ç±»åˆ«ID (0: mandatory, 1: prohibitory, 2: warning) è¾¹ç•Œæ¡†åæ ‡ (å½’ä¸€åŒ–çš„ä¸­å¿ƒç‚¹åæ ‡å’Œå®½é«˜) æ ¼å¼ï¼šclass_id center_x center_y width height 2.1.3 æ•°æ®é›†é…ç½®path: /path/to/data train: train/images val: test/images test: test/images nc: 3 names: ['mandatory', 'prohibitory', 'warning'] 2.2 æ¨¡å‹è®­ç»ƒé˜¶æ®µ2.2.1 è®­ç»ƒå‚æ•°é…ç½®# åŸºç¡€è®­ç»ƒå‚æ•° epochs: 10 # è®­ç»ƒè½®æ•° batch_size: 16 # æ‰¹æ¬¡å¤§å° img_size: 640 # è¾“å…¥å›¾åƒå°ºå¯¸ device: '0' # è®­ç»ƒè®¾å¤‡ (GPU/CPU) workers: 4 # æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•° # å­¦ä¹ ç‡é…ç½® lr0: 0.01 # åˆå§‹å­¦ä¹ ç‡ lrf: 0.01 # æœ€ç»ˆå­¦ä¹ ç‡ momentum: 0.937 # åŠ¨é‡ weight_decay: 0.0005 # æƒé‡è¡°å‡ warmup_epochs: 3.0 # é¢„çƒ­è½®æ•° # æŸå¤±å‡½æ•°æƒé‡ box: 7.5 # è¾¹ç•Œæ¡†æŸå¤±æƒé‡ cls: 0.5 # åˆ†ç±»æŸå¤±æƒé‡ dfl: 1.5 # DFLæŸå¤±æƒé‡ # æ•°æ®å¢å¼ºå‚æ•° hsv_h: 0.015 # HSVè‰²è°ƒå¢å¼º hsv_s: 0.7 # HSVé¥±å’Œåº¦å¢å¼º hsv_v: 0.4 # HSVæ˜åº¦å¢å¼º degrees: 0.0 # æ—‹è½¬è§’åº¦ translate: 0.1 # å¹³ç§» scale: 0.5 # ç¼©æ”¾ fliplr: 0.5 # å·¦å³ç¿»è½¬ mosaic: 1.0 # é©¬èµ›å…‹å¢å¼º 2.2.2 è®­ç»ƒæµç¨‹ æ•°æ®åŠ è½½ï¼šä½¿ç”¨DataLoaderåŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ® å‰å‘ä¼ æ’­ï¼šæ¨¡å‹å¯¹è¾“å…¥å›¾åƒè¿›è¡Œç‰¹å¾æå–å’Œé¢„æµ‹ æŸå¤±è®¡ç®—ï¼šè®¡ç®—è¾¹ç•Œæ¡†æŸå¤±ã€åˆ†ç±»æŸå¤±å’ŒDFLæŸå¤± åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°æ¨¡å‹å‚æ•° éªŒè¯è¯„ä¼°ï¼šåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ æ¨¡å‹ä¿å­˜ï¼šä¿å­˜æœ€ä½³æ¨¡å‹å’Œæ£€æŸ¥ç‚¹ 2.2.3 è®­ç»ƒç›‘æ§ æŸå¤±æ›²çº¿ï¼šç›‘æ§è®­ç»ƒå’ŒéªŒè¯æŸå¤±çš„å˜åŒ– æ€§èƒ½æŒ‡æ ‡ï¼šmAP50ã€mAP50-95ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ æ··æ·†çŸ©é˜µï¼šåˆ†æå„ç±»åˆ«çš„æ£€æµ‹æ€§èƒ½ å­¦ä¹ ç‡è°ƒåº¦ï¼šä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ 2.3 æ¨¡å‹éªŒè¯é˜¶æ®µ2.3.1 éªŒè¯æŒ‡æ ‡ mAP50ï¼šIoUé˜ˆå€¼ä¸º0.5æ—¶çš„å¹³å‡ç²¾åº¦ mAP50-95ï¼šIoUé˜ˆå€¼ä»0.5åˆ°0.95çš„å¹³å‡ç²¾åº¦ **ç²¾ç¡®ç‡(Precision)**ï¼šæ­£ç¡®æ£€æµ‹çš„é˜³æ€§æ ·æœ¬æ¯”ä¾‹ **å¬å›ç‡(Recall)**ï¼šæ­£ç¡®æ£€æµ‹çš„çœŸå®é˜³æ€§æ ·æœ¬æ¯”ä¾‹ F1åˆ†æ•°ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•° 2.3.2 éªŒè¯æµç¨‹ åŠ è½½è®­ç»ƒå¥½çš„æœ€ä½³æ¨¡å‹ åœ¨éªŒè¯é›†ä¸Šè¿è¡Œæ¨ç† è®¡ç®—å„ç§æ€§èƒ½æŒ‡æ ‡ ç”Ÿæˆå¯è§†åŒ–ç»“æœï¼ˆæ··æ·†çŸ©é˜µã€PRæ›²çº¿ç­‰ï¼‰ ä¿å­˜éªŒè¯ç»“æœå’ŒæŠ¥å‘Š 2.4 æ¨¡å‹å¯¼å‡ºé˜¶æ®µ2.4.1 å¯¼å‡ºæ ¼å¼ **PyTorchæ ¼å¼(.pt)**ï¼šåŸå§‹è®­ç»ƒæ ¼å¼ï¼Œç”¨äºç»§ç»­è®­ç»ƒ **ONNXæ ¼å¼(.onnx)**ï¼šè·¨å¹³å°éƒ¨ç½²æ ¼å¼ **TensorRTæ ¼å¼(.engine)**ï¼šGPUåŠ é€Ÿæ¨ç†æ ¼å¼ 2.4.2 å¯¼å‡ºæµç¨‹ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ è½¬æ¢ä¸ºç›®æ ‡æ ¼å¼ éªŒè¯å¯¼å‡ºæ¨¡å‹çš„æ­£ç¡®æ€§ ä¿å­˜åˆ°æŒ‡å®šç›®å½• ä¸‰ã€æ•°æ®é›†è¯¦è§£3.1 æ•°æ®é›†æ¥æºæœ¬é¡¹ç›®ä½¿ç”¨TSRD (Traffic Sign Recognition Dataset) æ•°æ®é›†ï¼ŒåŒ…å«ï¼š è®­ç»ƒé›†ï¼š16,356å¼ å›¾ç‰‡ï¼Œè¦†ç›–å„ç§äº¤é€šæ ‡å¿— æµ‹è¯•é›†ï¼š1,500å¼ å›¾ç‰‡ï¼Œç”¨äºæ¨¡å‹è¯„ä¼° ç±»åˆ«ï¼š3ç±»äº¤é€šæ ‡å¿—ï¼ˆæŒ‡ç¤ºã€ç¦ä»¤ã€è­¦å‘Šï¼‰ 3.2 æ•°æ®é¢„å¤„ç†3.2.1 å›¾åƒé¢„å¤„ç† å°ºå¯¸è°ƒæ•´ï¼šç»Ÿä¸€è°ƒæ•´ä¸º640x640åƒç´  å½’ä¸€åŒ–ï¼šåƒç´ å€¼å½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´ æ•°æ®å¢å¼ºï¼šæ—‹è½¬ã€ç¿»è½¬ã€ç¼©æ”¾ã€é¢œè‰²å˜æ¢ç­‰ 3.2.2 æ ‡ç­¾å¤„ç† æ ¼å¼è½¬æ¢ï¼šå°†è¾¹ç•Œæ¡†åæ ‡è½¬æ¢ä¸ºYOLOæ ¼å¼ ç±»åˆ«æ˜ å°„ï¼šå°†åŸå§‹ç±»åˆ«æ˜ å°„åˆ°0,1,2 åæ ‡å½’ä¸€åŒ–ï¼šå°†åƒç´ åæ ‡è½¬æ¢ä¸ºç›¸å¯¹åæ ‡ 3.3 æ•°æ®å¢å¼ºç­–ç•¥3.3.1 å‡ ä½•å˜æ¢ æ—‹è½¬ï¼šéšæœºæ—‹è½¬Â±15åº¦ ç¿»è½¬ï¼šæ°´å¹³ç¿»è½¬ï¼Œæ¦‚ç‡0.5 ç¼©æ”¾ï¼šéšæœºç¼©æ”¾0.5-1.5å€ å¹³ç§»ï¼šéšæœºå¹³ç§»Â±10% 3.3.2 é¢œè‰²å˜æ¢ HSVè°ƒæ•´ï¼šè‰²è°ƒÂ±1.5%ï¼Œé¥±å’Œåº¦Â±70%ï¼Œæ˜åº¦Â±40% äº®åº¦å¯¹æ¯”åº¦ï¼šéšæœºè°ƒæ•´äº®åº¦å’Œå¯¹æ¯”åº¦ å™ªå£°æ·»åŠ ï¼šæ·»åŠ é«˜æ–¯å™ªå£° 3.3.3 é«˜çº§å¢å¼º é©¬èµ›å…‹å¢å¼ºï¼šå°†4å¼ å›¾ç‰‡æ‹¼æ¥æˆ1å¼  æ··åˆå¢å¼ºï¼šå°†ä¸¤å¼ å›¾ç‰‡æ··åˆ Copy-Pasteï¼šå¤åˆ¶ç²˜è´´å¢å¼º å››ã€ä¼˜åŒ–ä»£ç è¯¦è§£4.1 ä¼˜åŒ–ç­–ç•¥æ¦‚è¿°4.1.1 ä¸»è¦ä¼˜åŒ–æ–¹å‘ æ¼æ£€é—®é¢˜ä¿®å¤ï¼šé™ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä¼˜åŒ–æŸå¤±å‡½æ•°æƒé‡ å°ç›®æ ‡æ£€æµ‹ä¼˜åŒ–ï¼šé›†æˆæ³¨æ„åŠ›æœºåˆ¶å’Œç‰¹å¾èåˆ æ•°æ®å¢å¼ºå¹³è¡¡ï¼šä¿æŒç‰¹å¾å®Œæ•´æ€§çš„åŒæ—¶å¢åŠ å¤šæ ·æ€§ è®­ç»ƒç­–ç•¥ä¼˜åŒ–ï¼šç¨³å®šå­¦ä¹ ç‡ï¼Œå……åˆ†é¢„çƒ­ 4.1.2 æ ¸å¿ƒæ”¹è¿› ç½®ä¿¡åº¦é˜ˆå€¼ï¼šä»0.5é™ä½åˆ°0.1ï¼Œå‡å°‘æ¼æ£€ IoUé˜ˆå€¼ï¼šä»0.6é™ä½åˆ°0.5ï¼Œæé«˜å¬å›ç‡ åˆ†ç±»æŸå¤±æƒé‡ï¼šä»0.5æé«˜åˆ°1.5ï¼Œå‡å°‘æ¼æ£€ å­¦ä¹ ç‡ç­–ç•¥ï¼šé™ä½åˆå§‹å­¦ä¹ ç‡ï¼Œå¢åŠ é¢„çƒ­è½®æ•° 4.2 æ³¨æ„åŠ›æœºåˆ¶é›†æˆ4.2.1 SEæ³¨æ„åŠ›æœºåˆ¶class SEAttention(nn.Module): \"\"\"Squeeze-and-Excitationæ³¨æ„åŠ›æœºåˆ¶\"\"\" def __init__(self, channel, reduction=16): super(SEAttention, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.fc = nn.Sequential( nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(inplace=True), nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid() ) 4.2.2 CBAMæ³¨æ„åŠ›æœºåˆ¶class CBAM(nn.Module): \"\"\"Convolutional Block Attention Module\"\"\" def __init__(self, in_channels, reduction=16, kernel_size=7): super(CBAM, self).__init__() self.channel_attention = ChannelAttention(in_channels, reduction) self.spatial_attention = SpatialAttention(kernel_size) 4.2.3 å¤šå°ºåº¦ç‰¹å¾èåˆclass MultiScaleFeatureFusion(nn.Module): \"\"\"å¤šå°ºåº¦ç‰¹å¾èåˆæ¨¡å—\"\"\" def __init__(self, in_channels_list, out_channels=256): super(MultiScaleFeatureFusion, self).__init__() # ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ self.lateral_convs = nn.ModuleList() self.fpn_convs = nn.ModuleList() # æ³¨æ„åŠ›æœºåˆ¶ç”¨äºç‰¹å¾èåˆ self.attention = CBAM(out_channels, reduction=16) 4.3 ä¼˜åŒ–è®­ç»ƒå‚æ•°4.3.1 æ£€æµ‹å‚æ•°ä¼˜åŒ–# ä¿®å¤æ¼æ£€çš„æ£€æµ‹å‚æ•° 'conf': 0.1, # é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œå‡å°‘æ¼æ£€ 'iou': 0.5, # é™ä½IoUé˜ˆå€¼ï¼Œæé«˜å¬å›ç‡ 'max_det': 300, # é€‚ä¸­çš„æœ€å¤§æ£€æµ‹æ•° 'multi_scale': True, # å¤šå°ºåº¦è®­ç»ƒ 'dropout': 0.0, # ç§»é™¤dropoutï¼Œé¿å…ç‰¹å¾ä¸¢å¤± 4.3.2 æ•°æ®å¢å¼ºä¼˜åŒ–# å¹³è¡¡çš„æ•°æ®å¢å¼º - ä¿æŒç‰¹å¾çš„åŒæ—¶å¢åŠ å¤šæ ·æ€§ 'hsv_h': 0.015, # é€‚ä¸­çš„è‰²è°ƒå˜åŒ– 'hsv_s': 0.3, # é€‚ä¸­çš„é¥±å’Œåº¦å˜åŒ–ï¼Œä¿æŒæ ‡å¿—é¢œè‰²ç‰¹å¾ 'hsv_v': 0.3, # é€‚ä¸­çš„æ˜åº¦å˜åŒ– 'degrees': 3.0, # é€‚ä¸­çš„æ—‹è½¬è§’åº¦ 'translate': 0.15, # é€‚ä¸­çš„å¹³ç§» 'scale': 0.3, # é€‚ä¸­çš„ç¼©æ”¾èŒƒå›´ï¼Œä¿æŒæ ‡å¿—å½¢çŠ¶ 'mosaic': 0.8, # é€‚ä¸­çš„é©¬èµ›å…‹å¢å¼º 'copy_paste': 0.2, # é€‚ä¸­çš„Copy-Pasteå¢å¼º 4.3.3 å­¦ä¹ ç‡ç­–ç•¥ä¼˜åŒ–# ä¿å®ˆçš„å­¦ä¹ ç‡ç­–ç•¥ 'lr0': 0.005, # é™ä½åˆå§‹å­¦ä¹ ç‡ï¼Œç¨³å®šè®­ç»ƒ 'lrf': 0.005, # é™ä½æœ€ç»ˆå­¦ä¹ ç‡ 'warmup_epochs': 5.0, # å¢åŠ é¢„çƒ­è½®æ•° 'cos_lr': True, # ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦ 'close_mosaic': 5, # æœ€å5ä¸ªepochå…³é—­é©¬èµ›å…‹ 4.4 ä¼˜åŒ–æ•ˆæœå¯¹æ¯”4.4.1 æ€§èƒ½æå‡ mAP50ï¼šä»0.557æå‡åˆ°0.764 (+37%) å¬å›ç‡ï¼šä»70%æå‡åˆ°85% (+15%) æ¼æ£€ç‡ï¼šä»40-50%é™ä½åˆ°20-30% (-50%) è¯¯æŠ¥ç‡ï¼šä»30-40%é™ä½åˆ°15-25% (-40%) 4.4.2 è®­ç»ƒç¨³å®šæ€§ æŸå¤±æ”¶æ•›ï¼šæ›´ç¨³å®šçš„æŸå¤±æ›²çº¿ æ¢¯åº¦ç¨³å®šï¼šé¿å…æ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤± å­¦ä¹ ç‡è°ƒåº¦ï¼šå¹³æ»‘çš„å­¦ä¹ ç‡å˜åŒ– äº”ã€GUIç•Œé¢è¯¦è§£5.1 ç•Œé¢è®¾è®¡5.1.1 æ•´ä½“å¸ƒå±€ å·¦ä¾§æ§åˆ¶é¢æ¿ï¼šæ¨¡å‹ç®¡ç†ã€æ£€æµ‹æ§åˆ¶ã€ç»“æœæ˜¾ç¤º å³ä¾§æ˜¾ç¤ºåŒºåŸŸï¼šå®æ—¶æ£€æµ‹æ˜¾ç¤ºã€ç»Ÿè®¡åˆ†æ åˆ†å‰²å™¨è®¾è®¡ï¼šå¯è°ƒæ•´å·¦å³é¢æ¿æ¯”ä¾‹ 5.1.2 ç°ä»£åŒ–UIç‰¹æ€§ æ¸å˜è‰²å½©ï¼šé‡‡ç”¨è“ç´«è‰²æ¸å˜èƒŒæ™¯ åœ†è§’è®¾è®¡ï¼šæ‰€æœ‰ç»„ä»¶é‡‡ç”¨åœ†è§’è¾¹æ¡† é˜´å½±æ•ˆæœï¼šæ·»åŠ ç«‹ä½“é˜´å½±æ•ˆæœ æ‚¬åœæ•ˆæœï¼šæŒ‰é’®æ‚¬åœæ—¶çš„è§†è§‰åé¦ˆ 5.2 åŠŸèƒ½æ¨¡å—5.2.1 æ¨¡å‹ç®¡ç† æ¨¡å‹åŠ è½½ï¼šè‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹ æ¨¡å‹é€‰æ‹©ï¼šæ”¯æŒé€‰æ‹©ä¸åŒæ¨¡å‹æ–‡ä»¶ çŠ¶æ€æ˜¾ç¤ºï¼šå®æ—¶æ˜¾ç¤ºæ¨¡å‹åŠ è½½çŠ¶æ€ 5.2.2 æ£€æµ‹æ§åˆ¶ ç½®ä¿¡åº¦è°ƒèŠ‚ï¼šæ»‘å—è°ƒèŠ‚ç½®ä¿¡åº¦é˜ˆå€¼ æ£€æµ‹æ¨¡å¼ï¼šæ‘„åƒå¤´ã€å›¾ç‰‡ã€è§†é¢‘ä¸‰ç§æ¨¡å¼ å®æ—¶æ§åˆ¶ï¼šå¼€å§‹/åœæ­¢æ£€æµ‹æŒ‰é’® 5.2.3 ç»“æœæ˜¾ç¤º å®æ—¶æ˜¾ç¤ºï¼šå¤§å°ºå¯¸å›¾åƒæ˜¾ç¤ºåŒºåŸŸ æ£€æµ‹ç»“æœï¼šè¯¦ç»†çš„æ£€æµ‹ä¿¡æ¯å±•ç¤º å†å²è®°å½•ï¼šæ£€æµ‹å†å²åˆ—è¡¨ ç»Ÿè®¡åˆ†æï¼šå„ç±»åˆ«æ£€æµ‹ç»Ÿè®¡ 5.3 å¤šçº¿ç¨‹è®¾è®¡5.3.1 æ£€æµ‹çº¿ç¨‹class DetectionThread(QThread): \"\"\"æ£€æµ‹çº¿ç¨‹ - é¿å…UIé˜»å¡\"\"\" detection_result = pyqtSignal(dict) # æ£€æµ‹ç»“æœä¿¡å· frame_ready = pyqtSignal(np.ndarray) # å¸§å°±ç»ªä¿¡å· finished = pyqtSignal() # å®Œæˆä¿¡å· 5.3.2 ä¿¡å·æ§½æœºåˆ¶ å¼‚æ­¥é€šä¿¡ï¼šä½¿ç”¨Qtä¿¡å·æ§½æœºåˆ¶ æ•°æ®ä¼ é€’ï¼šæ£€æµ‹ç»“æœå’Œå›¾åƒæ•°æ®ä¼ é€’ çŠ¶æ€åŒæ­¥ï¼šUIçŠ¶æ€ä¸æ£€æµ‹çŠ¶æ€åŒæ­¥ 5.4 å›¾åƒå¤„ç†ä¼˜åŒ–5.4.1 æ˜¾ç¤ºä¼˜åŒ–def update_display(self, frame): \"\"\"æ›´æ–°æ˜¾ç¤º - ä¼˜åŒ–å›¾åƒæ˜¾ç¤ºå¤§å°å’Œæ¸…æ™°åº¦\"\"\" # è½¬æ¢BGRåˆ°RGB rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # ä¼˜åŒ–æ˜¾ç¤ºå°ºå¯¸ - è®©å›¾åƒæ˜¾ç¤ºæ›´å¤§ display_width = 1200 # å¢åŠ æ˜¾ç¤ºå®½åº¦ display_height = 900 # å¢åŠ æ˜¾ç¤ºé«˜åº¦ # é«˜è´¨é‡ç¼©æ”¾ scaled_pixmap = pixmap.scaled( new_width, new_height, Qt.KeepAspectRatio, Qt.SmoothTransformation ) 5.4.2 æ£€æµ‹è¦†ç›–å±‚def _add_detection_overlay(self, frame): \"\"\"æ·»åŠ æ£€æµ‹ä¿¡æ¯è¦†ç›–å±‚\"\"\" if hasattr(self, 'last_detections') and self.last_detections: # åœ¨å›¾åƒä¸Šæ·»åŠ æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯ overlay_text = f\"æ£€æµ‹åˆ° {len(self.last_detections)} ä¸ªäº¤é€šæ ‡å¿—\" cv2.putText(frame, overlay_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2) å…­ã€ä»£ç ç»“æ„è¯¦è§£6.1 é¡¹ç›®ç›®å½•ç»“æ„yolo-v11-traffic-sign-main/ â”œâ”€â”€ backend/ # åç«¯ä»£ç  â”‚ â”œâ”€â”€ config.py # é…ç½®æ–‡ä»¶ â”‚ â”œâ”€â”€ detect_yolov11.py # æ£€æµ‹å™¨ â”‚ â”œâ”€â”€ train_yolov11.py # åŸå§‹è®­ç»ƒè„šæœ¬ â”‚ â””â”€â”€ detect_yolov11_enhanced.py # å¢å¼ºæ£€æµ‹å™¨ â”œâ”€â”€ data/ # æ•°æ®é›† â”‚ â”œâ”€â”€ train/ # è®­ç»ƒæ•°æ® â”‚ â”œâ”€â”€ test/ # æµ‹è¯•æ•°æ® â”‚ â””â”€â”€ dataset.yaml # æ•°æ®é›†é…ç½® â”œâ”€â”€ models/ # æ¨¡å‹æ–‡ä»¶ â”‚ â”œâ”€â”€ best_yolov11.pt # åŸå§‹æœ€ä½³æ¨¡å‹ â”‚ â””â”€â”€ best_yolov11_optimized.pt # ä¼˜åŒ–æ¨¡å‹ â”œâ”€â”€ results/ # è®­ç»ƒç»“æœ â”œâ”€â”€ frontend/ # å‰ç«¯ä»£ç  â”œâ”€â”€ gui_traffic_detector.py # GUIä¸»ç¨‹åº â”œâ”€â”€ train_yolov11_optimized.py # ä¼˜åŒ–è®­ç»ƒè„šæœ¬ â””â”€â”€ requirements.txt # ä¾èµ–åŒ… 6.2 æ ¸å¿ƒç±»è®¾è®¡6.2.1 YOLOv11Trainerç±»class YOLOv11Trainer: \"\"\"åŸå§‹è®­ç»ƒå™¨\"\"\" def __init__(self, config) # åˆå§‹åŒ– def prepare_dataset(self) # æ•°æ®å‡†å¤‡ def train_model(self) # æ¨¡å‹è®­ç»ƒ def validate_model(self) # æ¨¡å‹éªŒè¯ def export_model(self) # æ¨¡å‹å¯¼å‡º 6.2.2 YOLOv11OptimizedTrainerç±»class YOLOv11OptimizedTrainer: \"\"\"ä¼˜åŒ–è®­ç»ƒå™¨\"\"\" def __init__(self, config) # åˆå§‹åŒ– def prepare_dataset(self) # æ•°æ®å‡†å¤‡ def _apply_attention_mechanisms(self) # åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶ def train_model(self) # ä¼˜åŒ–è®­ç»ƒ def validate_model(self) # æ¨¡å‹éªŒè¯ def export_model(self) # æ¨¡å‹å¯¼å‡º 6.2.3 TrafficSignDetectorGUIç±»class TrafficSignDetectorGUI(QMainWindow): \"\"\"GUIä¸»çª—å£\"\"\" def __init__(self) # åˆå§‹åŒ– def init_ui(self) # åˆå§‹åŒ–UI def create_control_panel(self) # åˆ›å»ºæ§åˆ¶é¢æ¿ def create_display_area(self) # åˆ›å»ºæ˜¾ç¤ºåŒºåŸŸ def load_model(self) # åŠ è½½æ¨¡å‹ def start_camera_detection(self) # å¼€å§‹æ‘„åƒå¤´æ£€æµ‹ def update_display(self, frame) # æ›´æ–°æ˜¾ç¤º def update_detection_result(self, result) # æ›´æ–°æ£€æµ‹ç»“æœ 6.3 å…³é”®å‡½æ•°è¯¦è§£6.3.1 æ•°æ®å‡†å¤‡å‡½æ•°def prepare_dataset(self): \"\"\"å‡†å¤‡æ•°æ®é›†ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®\"\"\" # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨ # åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶ # å»ºç«‹æ ‡ç­¾æ–‡ä»¶é“¾æ¥ 6.3.2 è®­ç»ƒå‡½æ•°def train_model(self): \"\"\"è®­ç»ƒYOLOv11æ¨¡å‹\"\"\" # åˆå§‹åŒ–æ¨¡å‹ # é…ç½®è®­ç»ƒå‚æ•° # æ‰§è¡Œè®­ç»ƒ # ä¿å­˜æœ€ä½³æ¨¡å‹ 6.3.3 æ£€æµ‹å‡½æ•°def detect_image(self, image, filename=None): \"\"\"æ£€æµ‹å•å¼ å›¾ç‰‡\"\"\" # å›¾åƒé¢„å¤„ç† # æ¨¡å‹æ¨ç† # åå¤„ç† # ç»˜åˆ¶æ£€æµ‹ç»“æœ ä¸ƒã€ä½¿ç”¨æŒ‡å—7.1 ç¯å¢ƒé…ç½®7.1.1 ç³»ç»Ÿè¦æ±‚ æ“ä½œç³»ç»Ÿï¼šWindows 10/11, Linux, macOS Pythonç‰ˆæœ¬ï¼šPython 3.8+ GPUï¼šNVIDIA GPU (æ¨èï¼Œç”¨äºåŠ é€Ÿè®­ç»ƒ) å†…å­˜ï¼š8GB+ RAM å­˜å‚¨ï¼š10GB+ å¯ç”¨ç©ºé—´ 7.1.2 ä¾èµ–å®‰è£…# å®‰è£…åŸºç¡€ä¾èµ– pip install -r requirements.txt # å®‰è£…PyTorch (GPUç‰ˆæœ¬) pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # å®‰è£…Ultralytics pip install ultralytics # å®‰è£…PyQt5 pip install PyQt5 7.2 è®­ç»ƒä½¿ç”¨7.2.1 åŸå§‹è®­ç»ƒ# åŸºç¡€è®­ç»ƒ python backend/train_yolov11.py --epochs 10 --batch-size 16 --img-size 640 # è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ python backend/train_yolov11.py --epochs 20 --batch-size 8 --img-size 1280 --device 0 7.2.2 ä¼˜åŒ–è®­ç»ƒ# ä¼˜åŒ–è®­ç»ƒ python train_yolov11_optimized.py --epochs 10 --batch-size 8 --img-size 640 # é«˜åˆ†è¾¨ç‡è®­ç»ƒ python train_yolov11_optimized.py --epochs 15 --batch-size 4 --img-size 1280 7.3 GUIä½¿ç”¨7.3.1 å¯åŠ¨GUI# å¯åŠ¨å›¾å½¢ç•Œé¢ python gui_traffic_detector.py 7.3.2 åŠŸèƒ½ä½¿ç”¨ æ¨¡å‹åŠ è½½ï¼šç‚¹å‡»â€é‡æ–°åŠ è½½æ¨¡å‹â€æˆ–â€é€‰æ‹©æ¨¡å‹æ–‡ä»¶â€ æ‘„åƒå¤´æ£€æµ‹ï¼šç‚¹å‡»â€ğŸ“· æ‘„åƒå¤´æ£€æµ‹â€ å›¾ç‰‡æ£€æµ‹ï¼šç‚¹å‡»â€ğŸ–¼ï¸ å›¾ç‰‡æ£€æµ‹â€ï¼Œé€‰æ‹©å›¾ç‰‡æ–‡ä»¶ è§†é¢‘æ£€æµ‹ï¼šç‚¹å‡»â€ğŸ¬ è§†é¢‘æ£€æµ‹â€ï¼Œé€‰æ‹©è§†é¢‘æ–‡ä»¶ å‚æ•°è°ƒèŠ‚ï¼šä½¿ç”¨ç½®ä¿¡åº¦æ»‘å—è°ƒèŠ‚æ£€æµ‹é˜ˆå€¼ æŸ¥çœ‹ç»“æœï¼šåœ¨â€æ£€æµ‹ç»“æœâ€åŒºåŸŸæŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ ç»Ÿè®¡åˆ†æï¼šåˆ‡æ¢åˆ°â€ğŸ“Š æ•°æ®åˆ†æâ€æ ‡ç­¾é¡µæŸ¥çœ‹ç»Ÿè®¡ 7.4 æ¨¡å‹éƒ¨ç½²7.4.1 æ¨¡å‹å¯¼å‡º# å¯¼å‡ºONNXæ ¼å¼ model.export(format='onnx', imgsz=640) # å¯¼å‡ºTensorRTæ ¼å¼ model.export(format='engine', imgsz=640) 7.4.2 æ¨ç†ä½¿ç”¨# åŠ è½½æ¨¡å‹ detector = YOLOv11Detector('best_yolov11.pt') # æ£€æµ‹å›¾ç‰‡ image = cv2.imread('test.jpg') result, detections = detector.detect_image(image) å…«ã€æ€§èƒ½ä¼˜åŒ–å»ºè®®8.1 è®­ç»ƒä¼˜åŒ–8.1.1 ç¡¬ä»¶ä¼˜åŒ– GPUé€‰æ‹©ï¼šä½¿ç”¨NVIDIA RTX 3080/4080æˆ–æ›´é«˜ å†…å­˜é…ç½®ï¼šè‡³å°‘16GB RAMï¼Œæ¨è32GB å­˜å‚¨ä¼˜åŒ–ï¼šä½¿ç”¨SSDå­˜å‚¨æ•°æ®é›† 8.1.2 å‚æ•°è°ƒä¼˜ æ‰¹æ¬¡å¤§å°ï¼šæ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼Œå»ºè®®8-16 å­¦ä¹ ç‡ï¼šä½¿ç”¨å­¦ä¹ ç‡æŸ¥æ‰¾å™¨æ‰¾åˆ°æœ€ä¼˜å€¼ æ•°æ®å¢å¼ºï¼šæ ¹æ®æ•°æ®é›†ç‰¹ç‚¹è°ƒæ•´å¢å¼ºå‚æ•° 8.2 æ¨ç†ä¼˜åŒ–8.2.1 æ¨¡å‹ä¼˜åŒ– æ¨¡å‹é‡åŒ–ï¼šä½¿ç”¨INT8é‡åŒ–å‡å°‘æ¨¡å‹å¤§å° æ¨¡å‹å‰ªæï¼šç§»é™¤ä¸é‡è¦çš„è¿æ¥å’Œé€šé“ çŸ¥è¯†è’¸é¦ï¼šä½¿ç”¨å¤§æ¨¡å‹æŒ‡å¯¼å°æ¨¡å‹è®­ç»ƒ 8.2.2 æ¨ç†åŠ é€Ÿ TensorRTä¼˜åŒ–ï¼šä½¿ç”¨TensorRTè¿›è¡ŒGPUåŠ é€Ÿ ONNX Runtimeï¼šä½¿ç”¨ONNX Runtimeè¿›è¡ŒCPUæ¨ç† æ‰¹å¤„ç†ï¼šæ‰¹é‡å¤„ç†å¤šå¼ å›¾ç‰‡ 8.3 ç³»ç»Ÿä¼˜åŒ–8.3.1 å†…å­˜ä¼˜åŒ– æ•°æ®åŠ è½½ï¼šä½¿ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½ ç¼“å­˜ç­–ç•¥ï¼šç¼“å­˜å¸¸ç”¨æ•°æ®åˆ°å†…å­˜ åƒåœ¾å›æ”¶ï¼šå®šæœŸæ¸…ç†æ— ç”¨å¯¹è±¡ 8.3.2 å¹¶å‘ä¼˜åŒ– å¤šçº¿ç¨‹ï¼šä½¿ç”¨å¤šçº¿ç¨‹å¤„ç†ä¸åŒä»»åŠ¡ å¼‚æ­¥å¤„ç†ï¼šä½¿ç”¨å¼‚æ­¥I/Oæé«˜æ•ˆç‡ é˜Ÿåˆ—ç®¡ç†ï¼šä½¿ç”¨é˜Ÿåˆ—ç®¡ç†ä»»åŠ¡è°ƒåº¦","link":"/2025/10/01/Yolo11%E4%BA%A4%E9%80%9A%E6%A0%87%E5%BF%97%E6%A3%80%E6%B5%8B%E7%B3%BB%E7%BB%9F/"},{"title":"Hello World","text":"hiä½ å¥½ï¼Œè¿™å°±æ˜¯æˆ‘~ å…³äºæˆ‘ä¸ªäººä¿¡æ¯ å§“åï¼šHuangZhongqi ä¸“ä¸šï¼šäººå·¥æ™ºèƒ½ é‚®ç®±ï¼šhuangzhongqi978@gmail.com GitHubï¼šhttps://github.com/Huangzhongqi978 ä¸ªäººåšå®¢ï¼šhttps://Huangzhongqi978.github.io å­¦ä¹ è¯¾ç¨‹æˆ‘åœ¨äººå·¥æ™ºèƒ½ä¸“ä¸šæœŸé—´å­¦ä¹ äº†ä»¥ä¸‹ä¸»è¦è¯¾ç¨‹ï¼š æ•°å­¦ä¸åŸºç¡€è¯¾ç¨‹ é«˜ç­‰æ•°å­¦ çº¿æ€§ä»£æ•° æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ ç¦»æ•£æ•°å­¦ è®¡ç®—æœºåŸºç¡€è¯¾ç¨‹ æ•°æ®ç»“æ„ä¸ç®—æ³• è®¡ç®—æœºç»„æˆåŸç† æ“ä½œç³»ç»Ÿ æ•°æ®åº“ç³»ç»Ÿ äººå·¥æ™ºèƒ½æ ¸å¿ƒè¯¾ç¨‹ æœºå™¨å­¦ä¹  æ·±åº¦å­¦ä¹  è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰ å¼ºåŒ–å­¦ä¹  å¤§æ•°æ®åˆ†æä¸å¤„ç† å‰æ²¿ä¸å®è·µè¯¾ç¨‹ æœºå™¨äººä¸åµŒå…¥å¼ç³»ç»Ÿ AI é¡¹ç›®å®è®­ äº‘è®¡ç®—ä¸äººå·¥æ™ºèƒ½å¹³å° ä¸“ä¸šæŠ€èƒ½ ç¼–ç¨‹è¯­è¨€ï¼š Pythonã€C++ã€Javaã€Matlab æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼š PyTorchã€TensorFlowã€Keras è‡ªç„¶è¯­è¨€å¤„ç†ï¼š æ–‡æœ¬åˆ†ç±»ã€çŸ¥è¯†å›¾è°±ã€é—®ç­”ç³»ç»Ÿã€è¯­éŸ³è¯†åˆ« è®¡ç®—æœºè§†è§‰ï¼š å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ï¼ˆYOLOã€Mask R-CNNï¼‰ã€å›¾åƒåˆ†å‰² å·¥å…·ä¸å¹³å°ï¼š Git/GitHubã€Dockerã€Linuxã€VS Codeã€PyCharm æ•°æ®å¤„ç†ï¼š NumPyã€Pandasã€Matplotlibã€Seabornã€OpenCV é¡¹ç›®ç»å†ï¼ˆç¤ºä¾‹ï¼‰ åŸºäºYOLOçš„æ™ºèƒ½äº¤é€šç›‘æ§ç³»ç»Ÿ ä½¿ç”¨ PyTorch è®­ç»ƒå°ç›®æ ‡æ£€æµ‹æ¨¡å‹ å®ç°å¯¹è½¦è¾†ç±»å‹å’Œæ•°é‡çš„å®æ—¶ç›‘æ§ éƒ¨ç½²åœ¨æœ¬åœ°æœåŠ¡å™¨ï¼Œæ”¯æŒå®æ—¶è§†é¢‘æµåˆ†æ è‡ªç„¶è¯­è¨€å¤„ç†é—®ç­”ç³»ç»Ÿ æ„å»ºåŸºäºæ–‡æ¡£çš„é—®ç­”ç³»ç»Ÿ ä½¿ç”¨ Bert å’Œ GPT æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç†è§£ æ”¯æŒå¤šè½®å¯¹è¯å’ŒçŸ¥è¯†å›¾è°±æ£€ç´¢ æ™ºèƒ½å¥åº·é—®è¯Šç³»ç»Ÿ æ•°æ®é‡è¶…è¿‡ 50,000 æ¡åŒ»ç–—é—®è¯Šè®°å½• å‰ç«¯ä½¿ç”¨ Vue.jsï¼Œåç«¯ Python Django å®ç°è‡ªåŠ¨é—®ç­”ã€ç—‡çŠ¶æ¨èã€æ¨¡å‹è¯„ä¼°ä¸å¯è§†åŒ– ä¸ªäººä¼˜åŠ¿ æ‰å®çš„æ•°å­¦å’Œç¼–ç¨‹åŸºç¡€ ç†Ÿæ‚‰æ·±åº¦å­¦ä¹ ä¸ NLP/CV å®è·µ èƒ½å¤Ÿç‹¬ç«‹å®Œæˆæ•°æ®åˆ†æä¸æ¨¡å‹éƒ¨ç½² å…·æœ‰è‰¯å¥½çš„å›¢é˜Ÿåä½œä¸é¡¹ç›®ç®¡ç†èƒ½åŠ› çƒ­è¡·äºå­¦ä¹ å‰æ²¿ AI æŠ€æœ¯ æœªæ¥è§„åˆ’ æ·±å…¥ç ”ç©¶è‡ªç„¶è¯­è¨€å¤„ç†ä¸ç”Ÿæˆå¼ AI å‚ä¸å¼€æº AI é¡¹ç›®ï¼Œè´¡çŒ®è‡ªå·±çš„ä»£ç  æ‰“é€ ä¸ªäººæŠ€æœ¯åšå®¢å’ŒçŸ¥è¯†åº“ äº‰å–åœ¨ AI ç›¸å…³é¢†åŸŸå®ç°ç§‘ç ”ä¸åº”ç”¨ç»“åˆ","link":"/2025/10/01/hello-world/"},{"title":"åŸºäºGRUçš„è¯­éŸ³æƒ…æ„Ÿåˆ†æè¯†åˆ«","text":"GRUæ¶æ„ä¸‹çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ« æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹ä¼˜åŒ–è¯¦ç»†æ–‡æ¡£ğŸ“‹ ç›®å½• æ¦‚è¿° æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯ ä»£ç å®ç°è¯¦è§£ å‚æ•°é…ç½®è¯´æ˜ è®­ç»ƒç­–ç•¥ä¼˜åŒ– æ€§èƒ½ç›‘æ§ä¸å¯è§†åŒ– ğŸ“– æ¦‚è¿°æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†IEMOCAPæƒ…æ„Ÿè¯†åˆ«æ¨¡å‹çš„æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯ï¼Œä¸»è¦è§£å†³è·¨è¯´è¯äººæƒ…æ„Ÿè¯†åˆ«ä¸­çš„æ³›åŒ–èƒ½åŠ›é—®é¢˜ã€‚ä¼˜åŒ–ç­–ç•¥åŒ…æ‹¬è¯´è¯äººæ— å…³åŒ–æŠ€æœ¯ã€é«˜çº§è®­ç»ƒç­–ç•¥å’Œç»¼åˆæŸå¤±å‡½æ•°è®¾è®¡ã€‚ ä¸»è¦ä¼˜åŒ–ç›®æ ‡ ğŸ¯ æå‡è·¨è¯´è¯äººæ³›åŒ–èƒ½åŠ›ï¼šæ¶ˆé™¤è¯´è¯äººç‰¹å¾å¯¹æƒ…æ„Ÿè¯†åˆ«çš„å¹²æ‰° ğŸ“ˆ å¢å¼ºæ¨¡å‹é²æ£’æ€§ï¼šé€šè¿‡å¤šç§æ­£åˆ™åŒ–å’Œæ•°æ®å¢å¼ºæŠ€æœ¯ âš¡ ä¼˜åŒ–è®­ç»ƒæ•ˆç‡ï¼šé‡‡ç”¨å…ˆè¿›çš„å­¦ä¹ ç‡è°ƒåº¦å’Œæ—©åœç­–ç•¥ ğŸ” æä¾›å…¨é¢ç›‘æ§ï¼šå®æ—¶å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹å’Œæ¨¡å‹æ€§èƒ½ ğŸ”§ æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯1. è¯´è¯äººæ— å…³åŒ–æŠ€æœ¯1.1 è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ– (AdaIN)åŸç†ï¼šé€šè¿‡å®ä¾‹çº§åˆ«çš„å½’ä¸€åŒ–æ¶ˆé™¤ä¸åŒè¯´è¯äººçš„éŸ³é¢‘ç‰¹å¾å·®å¼‚ï¼Œä¿ç•™æƒ…æ„Ÿç›¸å…³ä¿¡æ¯ã€‚ class AdaptiveInstanceNormalization(nn.Module): \"\"\" è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ– - è¯´è¯äººå½’ä¸€åŒ–å±‚ åŸç†ï¼š 1. è®¡ç®—æ¯ä¸ªæ ·æœ¬åœ¨æ—¶åºç»´åº¦ä¸Šçš„å‡å€¼å’Œæ–¹å·® 2. è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œæ¶ˆé™¤è¯´è¯äººç‰¹å¾å·®å¼‚ 3. é€šè¿‡å¯å­¦ä¹ å‚æ•°é‡æ–°ç¼©æ”¾ï¼Œä¿ç•™æƒ…æ„Ÿä¿¡æ¯ æ•°å­¦å…¬å¼ï¼š Î¼ = mean(x, dim=1) # æ—¶åºç»´åº¦å‡å€¼ ÏƒÂ² = var(x, dim=1) # æ—¶åºç»´åº¦æ–¹å·® x_norm = (x - Î¼) / âˆš(ÏƒÂ² + Îµ) # å½’ä¸€åŒ– output = Î³ * x_norm + Î² # ä»¿å°„å˜æ¢ \"\"\" def __init__(self, num_features, eps=1e-5): super(AdaptiveInstanceNormalization, self).__init__() self.num_features = num_features self.eps = eps # æ•°å€¼ç¨³å®šæ€§å‚æ•° # å¯å­¦ä¹ çš„ç¼©æ”¾å’Œåç§»å‚æ•° self.weight = nn.Parameter(torch.ones(num_features)) # Î³ ç¼©æ”¾å‚æ•° self.bias = nn.Parameter(torch.zeros(num_features)) # Î² åç§»å‚æ•° def forward(self, x): \"\"\" å‰å‘ä¼ æ’­ Args: x: [batch_size, seq_len, num_features] è¾“å…¥ç‰¹å¾ Returns: å½’ä¸€åŒ–åçš„ç‰¹å¾ \"\"\" # è®¡ç®—å®ä¾‹çº§åˆ«çš„å‡å€¼å’Œæ–¹å·®ï¼ˆè·¨åºåˆ—ç»´åº¦ï¼‰ mean = x.mean(dim=1, keepdim=True) # [batch_size, 1, num_features] var = x.var(dim=1, keepdim=True, unbiased=False) # [batch_size, 1, num_features] # å½’ä¸€åŒ–å¤„ç† x_norm = (x - mean) / torch.sqrt(var + self.eps) # åº”ç”¨å¯å­¦ä¹ çš„ä»¿å°„å˜æ¢ out = x_norm * self.weight.unsqueeze(0).unsqueeze(0) + self.bias.unsqueeze(0).unsqueeze(0) return out å…³é”®å‚æ•°ï¼š num_features: ç‰¹å¾ç»´åº¦æ•°é‡ eps: æ•°å€¼ç¨³å®šæ€§å‚æ•°ï¼Œé˜²æ­¢é™¤é›¶é”™è¯¯ weight: å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°Î³ bias: å¯å­¦ä¹ çš„åç§»å‚æ•°Î² 1.2 æ¢¯åº¦åè½¬å¯¹æŠ—è®­ç»ƒåŸç†ï¼šé€šè¿‡æ¢¯åº¦åè½¬å±‚å®ç°å¯¹æŠ—è®­ç»ƒï¼Œè¿«ä½¿æ¨¡å‹å­¦ä¹ è¯´è¯äººæ— å…³çš„ç‰¹å¾è¡¨ç¤ºã€‚ class GradientReversalLayer(torch.autograd.Function): \"\"\" æ¢¯åº¦åè½¬å±‚ - å¯¹æŠ—è®­ç»ƒæ ¸å¿ƒç»„ä»¶ åŸç†ï¼š 1. å‰å‘ä¼ æ’­ï¼šæ­£å¸¸ä¼ é€’ç‰¹å¾ï¼Œä¸åšä»»ä½•æ”¹å˜ 2. åå‘ä¼ æ’­ï¼šå°†æ¢¯åº¦ä¹˜ä»¥è´Ÿçš„ç¼©æ”¾å› å­Î± 3. æ•ˆæœï¼šä½¿æ¨¡å‹æ— æ³•ä»ç‰¹å¾ä¸­è¯†åˆ«è¯´è¯äººèº«ä»½ æ•°å­¦è¡¨ç¤ºï¼š forward: y = x backward: âˆ‚L/âˆ‚x = -Î± * âˆ‚L/âˆ‚y \"\"\" @staticmethod def forward(ctx, x, alpha): \"\"\" å‰å‘ä¼ æ’­ï¼šç›´æ¥ä¼ é€’è¾“å…¥ Args: x: è¾“å…¥ç‰¹å¾ alpha: æ¢¯åº¦åè½¬å¼ºåº¦ \"\"\" ctx.alpha = alpha # ä¿å­˜alphaç”¨äºåå‘ä¼ æ’­ return x.view_as(x) @staticmethod def backward(ctx, grad_output): \"\"\" åå‘ä¼ æ’­ï¼šæ¢¯åº¦ç¬¦å·åè½¬ Args: grad_output: æ¥è‡ªä¸Šå±‚çš„æ¢¯åº¦ Returns: åè½¬åçš„æ¢¯åº¦ \"\"\" return grad_output.neg() * ctx.alpha, None def gradient_reverse(x, alpha=1.0): \"\"\"æ¢¯åº¦åè½¬å‡½æ•°åŒ…è£…å™¨\"\"\" return GradientReversalLayer.apply(x, alpha) è¯´è¯äººåˆ†ç±»å™¨ï¼š # è¯´è¯äººåˆ†ç±»å¤´ï¼ˆç”¨äºå¯¹æŠ—è®­ç»ƒï¼‰ if self.use_adversarial: self.speaker_classifier = nn.Sequential( nn.Linear(hidden_size * 4, hidden_size), # ç‰¹å¾é™ç»´ nn.ReLU(inplace=True), # éçº¿æ€§æ¿€æ´» nn.Dropout(self.dropout_rate), # é˜²è¿‡æ‹Ÿåˆ nn.Linear(hidden_size, hidden_size // 2), # è¿›ä¸€æ­¥é™ç»´ nn.ReLU(inplace=True), nn.Dropout(self.dropout_rate), nn.Linear(hidden_size // 2, 10) # 10ä¸ªè¯´è¯äººåˆ†ç±» ) 2. å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶åŸç†ï¼šé€šè¿‡å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ•è·åºåˆ—ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œå¢å¼ºæƒ…æ„Ÿç‰¹å¾è¡¨ç¤ºã€‚ class MultiHeadSelfAttention(nn.Module): \"\"\" å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ åŸç†ï¼š 1. å°†è¾“å…¥ç‰¹å¾åˆ†åˆ«æŠ•å½±åˆ°Qã€Kã€Vç©ºé—´ 2. è®¡ç®—å¤šä¸ªæ³¨æ„åŠ›å¤´çš„æ³¨æ„åŠ›æƒé‡ 3. åŠ æƒèšåˆç‰¹å¾ä¿¡æ¯ 4. é€šè¿‡æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ç¨³å®šè®­ç»ƒ æ³¨æ„åŠ›å…¬å¼ï¼š Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V MultiHead = Concat(head_1, ..., head_h)W^O \"\"\" def __init__(self, d_model, num_heads, dropout=0.1): super(MultiHeadSelfAttention, self).__init__() assert d_model % num_heads == 0 self.d_model = d_model # æ¨¡å‹ç»´åº¦ self.num_heads = num_heads # æ³¨æ„åŠ›å¤´æ•°é‡ self.d_k = d_model // num_heads # æ¯ä¸ªå¤´çš„ç»´åº¦ # çº¿æ€§æŠ•å½±å±‚ self.w_q = nn.Linear(d_model, d_model) # QueryæŠ•å½± self.w_k = nn.Linear(d_model, d_model) # KeyæŠ•å½± self.w_v = nn.Linear(d_model, d_model) # ValueæŠ•å½± self.w_o = nn.Linear(d_model, d_model) # è¾“å‡ºæŠ•å½± # æ­£åˆ™åŒ–å±‚ self.dropout = nn.Dropout(dropout) self.layer_norm = nn.LayerNorm(d_model) def forward(self, x): \"\"\" å‰å‘ä¼ æ’­ Args: x: [batch_size, seq_len, d_model] è¾“å…¥ç‰¹å¾ Returns: æ³¨æ„åŠ›å¢å¼ºåçš„ç‰¹å¾å’Œæ³¨æ„åŠ›æƒé‡ \"\"\" batch_size, seq_len, d_model = x.size() # ä¿å­˜æ®‹å·®è¿æ¥ residual = x # 1. çº¿æ€§æŠ•å½±åˆ°Qã€Kã€V Q = self.w_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2) K = self.w_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2) V = self.w_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2) # 2. è®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) attention_weights = F.softmax(attention_scores, dim=-1) attention_weights = self.dropout(attention_weights) # 3. åŠ æƒèšåˆValue context = torch.matmul(attention_weights, V) # 4. æ‹¼æ¥å¤šå¤´è¾“å‡º context = context.transpose(1, 2).contiguous().view( batch_size, seq_len, self.d_model ) # 5. è¾“å‡ºæŠ•å½± output = self.w_o(context) # 6. æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ– output = self.layer_norm(output + residual) return output, attention_weights.mean(dim=1) # è¿”å›å¹³å‡æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§†åŒ– 3. ä½ç½®ç¼–ç class PositionalEncoding(nn.Module): \"\"\" æ­£å¼¦ä½ç½®ç¼–ç  åŸç†ï¼š ä½¿ç”¨ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ä¸ºåºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®ç”Ÿæˆå”¯ä¸€çš„ç¼–ç  å…¬å¼ï¼š PE(pos, 2i) = sin(pos / 10000^(2i/d_model)) PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model)) \"\"\" def __init__(self, d_model, max_length=5000): super(PositionalEncoding, self).__init__() # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ pe = torch.zeros(max_length, d_model) position = torch.arange(0, max_length).unsqueeze(1).float() # è®¡ç®—é™¤æ•°é¡¹ div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)) # åº”ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•° pe[:, 0::2] = torch.sin(position * div_term) # å¶æ•°ä½ç½®ä½¿ç”¨sin pe[:, 1::2] = torch.cos(position * div_term) # å¥‡æ•°ä½ç½®ä½¿ç”¨cos self.register_buffer('pe', pe.unsqueeze(0)) def forward(self, x): \"\"\"æ·»åŠ ä½ç½®ç¼–ç åˆ°è¾“å…¥ç‰¹å¾\"\"\" seq_len = x.size(1) pos_encoding = self.pe[:, :seq_len, :].to(x.device) return x + pos_encoding ğŸ—ï¸ ä»£ç å®ç°è¯¦è§£1. å¢å¼ºGRUæ¨¡å‹æ¶æ„class EnhancedGRUModel(nn.Module): \"\"\" å¢å¼ºçš„GRUæ¨¡å‹ - é’ˆå¯¹è·¨è¯´è¯äººæƒ…æ„Ÿè¯†åˆ«ä¼˜åŒ– æ¶æ„ç‰¹ç‚¹ï¼š 1. è¾“å…¥æŠ•å½± + ä½ç½®ç¼–ç  2. è¯´è¯äººå½’ä¸€åŒ–å±‚ (AdaIN) 3. å¤šå±‚åŒå‘GRU + å±‚å½’ä¸€åŒ– + æ®‹å·®è¿æ¥ 4. å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ 5. ç‰¹å¾å¢å¼ºæ¨¡å— 6. åŒè·¯åˆ†ç±»å¤´ï¼ˆæƒ…æ„Ÿ + è¯´è¯äººå¯¹æŠ—ï¼‰ \"\"\" def __init__(self, input_size, hidden_size, output_size, args): super(EnhancedGRUModel, self).__init__() # åŸºç¡€å‚æ•° self.input_size = input_size self.hidden_size = hidden_size self.output_size = output_size self.num_layers = args.dia_layers self.dropout_rate = args.dropout # åŠŸèƒ½å¼€å…³ self.use_attention = args.attention self.use_speaker_norm = getattr(args, 'speaker_norm', True) self.use_adversarial = getattr(args, 'speaker_adversarial', True) # 1. è¾“å…¥å¤„ç†å±‚ self.input_projection = nn.Linear(input_size, hidden_size * 2) self.pos_encoding = PositionalEncoding(hidden_size * 2) # 2. è¯´è¯äººå½’ä¸€åŒ–å±‚ if self.use_speaker_norm: self.speaker_norm = AdaptiveInstanceNormalization(hidden_size * 2) # 3. å¤šå±‚åŒå‘GRU self.gru_layers = nn.ModuleList() self.layer_norms = nn.ModuleList() for i in range(self.num_layers): input_dim = hidden_size * 2 if i == 0 else hidden_size * 4 self.gru_layers.append( nn.GRU(input_dim, hidden_size * 2, batch_first=True, bidirectional=True, dropout=self.dropout_rate if i &lt; self.num_layers-1 else 0) ) self.layer_norms.append(nn.LayerNorm(hidden_size * 4)) # 4. å¤šå¤´è‡ªæ³¨æ„åŠ› if self.use_attention: self.self_attention = MultiHeadSelfAttention( d_model=hidden_size * 4, num_heads=8, dropout=self.dropout_rate ) # 5. ç‰¹å¾å¢å¼ºæ¨¡å— self.feature_enhancement = nn.Sequential( nn.Linear(hidden_size * 4, hidden_size * 2), nn.ReLU(inplace=True), nn.Dropout(self.dropout_rate), nn.Linear(hidden_size * 2, hidden_size * 2), nn.ReLU(inplace=True), nn.Dropout(self.dropout_rate) ) # 6. å…¨å±€æ± åŒ–ç­–ç•¥ self.global_pooling = nn.AdaptiveAvgPool1d(1) # å¹³å‡æ± åŒ– self.global_max_pooling = nn.AdaptiveMaxPool1d(1) # æœ€å¤§æ± åŒ– # 7. æƒ…æ„Ÿåˆ†ç±»å¤´ self.emotion_classifier = nn.Sequential( nn.Linear(hidden_size * 4, hidden_size), nn.ReLU(inplace=True), nn.Dropout(self.dropout_rate), nn.Linear(hidden_size, hidden_size // 2), nn.ReLU(inplace=True), nn.Dropout(self.dropout_rate), nn.Linear(hidden_size // 2, output_size) ) # 8. è¯´è¯äººåˆ†ç±»å¤´ï¼ˆå¯¹æŠ—è®­ç»ƒï¼‰ if self.use_adversarial: self.speaker_classifier = nn.Sequential( nn.Linear(hidden_size * 4, hidden_size), nn.ReLU(inplace=True), nn.Dropout(self.dropout_rate), nn.Linear(hidden_size, hidden_size // 2), nn.ReLU(inplace=True), nn.Dropout(self.dropout_rate), nn.Linear(hidden_size // 2, 10) # IEMOCAPæœ‰10ä¸ªè¯´è¯äºº ) self.dropout = nn.Dropout(self.dropout_rate) self._init_weights() 2. å‰å‘ä¼ æ’­æµç¨‹def forward(self, x, alpha=1.0): \"\"\" å‰å‘ä¼ æ’­ Args: x: [batch_size, seq_len, input_size] è¾“å…¥ç‰¹å¾ alpha: æ¢¯åº¦åè½¬å¼ºåº¦ï¼ˆç”¨äºå¯¹æŠ—è®­ç»ƒï¼‰ Returns: dict: åŒ…å«æƒ…æ„Ÿå’Œè¯´è¯äººé¢„æµ‹ç»“æœçš„å­—å…¸ \"\"\" batch_size, seq_len, _ = x.size() # 1. è¾“å…¥æŠ•å½±å’Œä½ç½®ç¼–ç  x = self.input_projection(x) # [batch_size, seq_len, hidden_size*2] x = self.pos_encoding(x) # æ·»åŠ ä½ç½®ä¿¡æ¯ # 2. è¯´è¯äººå½’ä¸€åŒ–ï¼ˆæ¶ˆé™¤è¯´è¯äººç‰¹å¾ï¼‰ if self.use_speaker_norm: x = self.speaker_norm(x) x = self.dropout(x) # 3. å¤šå±‚åŒå‘GRUå¤„ç† for i, (gru_layer, layer_norm) in enumerate(zip(self.gru_layers, self.layer_norms)): residual = x if i > 0 else None gru_out, _ = gru_layer(x) # [batch_size, seq_len, hidden_size*4] gru_out = layer_norm(gru_out) # æ®‹å·®è¿æ¥ï¼ˆä»ç¬¬äºŒå±‚å¼€å§‹ï¼‰ if residual is not None and residual.size(-1) == gru_out.size(-1): gru_out = gru_out + residual x = self.dropout(gru_out) # 4. å¤šå¤´è‡ªæ³¨æ„åŠ›å¢å¼º attention_weights = None if self.use_attention: x, attention_weights = self.self_attention(x) # 5. ç‰¹å¾å¢å¼º enhanced_features = self.feature_enhancement(x) combined_features = torch.cat([x, enhanced_features], dim=-1) combined_features = combined_features[:, :, :self.hidden_size*4] # 6. å…¨å±€æ± åŒ– pooling_input = combined_features.transpose(1, 2) avg_pooled = self.global_pooling(pooling_input).squeeze(-1) max_pooled = self.global_max_pooling(pooling_input).squeeze(-1) # æ‹¼æ¥æ± åŒ–ç»“æœ pooled_features = torch.cat([avg_pooled, max_pooled], dim=1) final_features = pooled_features[:, :self.hidden_size*4] # 7. æƒ…æ„Ÿåˆ†ç±» emotion_logits = self.emotion_classifier(final_features) # 8. è¯´è¯äººå¯¹æŠ—åˆ†ç±» speaker_logits = None if self.use_adversarial: # åº”ç”¨æ¢¯åº¦åè½¬ adversarial_features = gradient_reverse(final_features, alpha) speaker_logits = self.speaker_classifier(adversarial_features) return { 'emotion_logits': emotion_logits, 'speaker_logits': speaker_logits, 'attention_weights': attention_weights, 'features': final_features } âš™ï¸ å‚æ•°é…ç½®è¯´æ˜1. æ¨¡å‹ç»“æ„å‚æ•°# åŸºç¡€æ¶æ„å‚æ•° input_size = 768 # HuBERTç‰¹å¾ç»´åº¦ hidden_size = 256 # GRUéšè—å±‚å¤§å° output_size = 4 # æƒ…æ„Ÿç±»åˆ«æ•°é‡ï¼ˆangry, happy, neutral, sadï¼‰ dia_layers = 3 # GRUå±‚æ•° # æ­£åˆ™åŒ–å‚æ•° dropout = 0.3 # Dropoutæ¦‚ç‡ max_grad_norm = 1.0 # æ¢¯åº¦è£å‰ªé˜ˆå€¼ l2_reg = 1e-5 # L2æ­£åˆ™åŒ–æƒé‡ 2. ä¼˜åŒ–ç­–ç•¥å‚æ•°# å­¦ä¹ ç‡è°ƒåº¦ learning_rate = 0.0005 # åˆå§‹å­¦ä¹ ç‡ lr_schedule = 'cosine' # å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ warmup_steps = 1000 # é¢„çƒ­æ­¥æ•° min_lr = 1e-7 # æœ€å°å­¦ä¹ ç‡ # è®­ç»ƒç­–ç•¥ batch_size = 32 # æ‰¹æ¬¡å¤§å° epochs = 50 # è®­ç»ƒè½®æ•° patience = 10 # æ—©åœè€å¿ƒå€¼ 3. è¯´è¯äººæ— å…³åŒ–å‚æ•°# AdaINå½’ä¸€åŒ– speaker_norm = True # å¯ç”¨è¯´è¯äººå½’ä¸€åŒ– eps = 1e-5 # æ•°å€¼ç¨³å®šæ€§å‚æ•° # å¯¹æŠ—è®­ç»ƒ speaker_adversarial = True # å¯ç”¨å¯¹æŠ—è®­ç»ƒ adversarial_weight = 0.05 # å¯¹æŠ—æŸå¤±æƒé‡ alpha_schedule = 'linear' # æ¢¯åº¦åè½¬å¼ºåº¦è°ƒåº¦ max_alpha = 1.0 # æœ€å¤§æ¢¯åº¦åè½¬å¼ºåº¦ 4. æ³¨æ„åŠ›æœºåˆ¶å‚æ•°# å¤šå¤´æ³¨æ„åŠ› attention = True # å¯ç”¨æ³¨æ„åŠ›æœºåˆ¶ num_heads = 8 # æ³¨æ„åŠ›å¤´æ•°é‡ attention_dropout = 0.1 # æ³¨æ„åŠ›dropout ğŸ¯ è®­ç»ƒç­–ç•¥ä¼˜åŒ–1. ç»¼åˆæŸå¤±å‡½æ•°def compute_loss(self, model_outputs, targets, speaker_targets, alpha=1.0): \"\"\" ç»¼åˆæŸå¤±å‡½æ•° ç»„æˆï¼š 1. ä¸»ä»»åŠ¡æŸå¤±ï¼šæƒ…æ„Ÿåˆ†ç±»äº¤å‰ç†µæŸå¤± 2. å¯¹æŠ—æŸå¤±ï¼šè¯´è¯äººæ··æ·†æŸå¤± 3. æ­£åˆ™åŒ–æŸå¤±ï¼šL2æƒé‡è¡°å‡ Args: model_outputs: æ¨¡å‹è¾“å‡ºå­—å…¸ targets: æƒ…æ„Ÿæ ‡ç­¾ speaker_targets: è¯´è¯äººæ ‡ç­¾ alpha: æ¢¯åº¦åè½¬å¼ºåº¦ Returns: total_loss: æ€»æŸå¤± loss_dict: å„é¡¹æŸå¤±è¯¦æƒ… \"\"\" emotion_logits = model_outputs['emotion_logits'] speaker_logits = model_outputs['speaker_logits'] # 1. æƒ…æ„Ÿåˆ†ç±»æŸå¤±ï¼ˆä¸»è¦ä»»åŠ¡ï¼‰ emotion_loss = F.cross_entropy(emotion_logits, targets) total_loss = emotion_loss loss_dict = {'emotion_loss': emotion_loss.item()} # 2. è¯´è¯äººå¯¹æŠ—æŸå¤± if speaker_logits is not None and self.args.speaker_adversarial: speaker_loss = F.cross_entropy(speaker_logits, speaker_targets) total_loss += self.args.adversarial_weight * speaker_loss loss_dict['speaker_loss'] = speaker_loss.item() # 3. æ­£åˆ™åŒ–æŸå¤± if self.args.l2_reg > 0: l2_loss = sum(torch.norm(p, 2) for p in model_outputs.get('regularization_params', [])) total_loss += self.args.l2_reg * l2_loss loss_dict['l2_loss'] = l2_loss.item() if isinstance(l2_loss, torch.Tensor) else l2_loss loss_dict['total_loss'] = total_loss.item() return total_loss, loss_dict 2. åŠ¨æ€å¯¹æŠ—è®­ç»ƒç­–ç•¥def get_alpha_schedule(self, epoch, total_epochs): \"\"\" åŠ¨æ€è°ƒæ•´æ¢¯åº¦åè½¬å¼ºåº¦ ç­–ç•¥ï¼š 1. å‰æœŸï¼ˆepoch &lt; 5ï¼‰ï¼šÎ± = 0ï¼Œä¸“æ³¨æƒ…æ„Ÿåˆ†ç±» 2. ä¸­æœŸï¼ˆ5 â‰¤ epoch &lt; total_epochs*0.7ï¼‰ï¼šçº¿æ€§å¢é•¿ 3. åæœŸï¼šä¿æŒæœ€å¤§å€¼ \"\"\" if epoch &lt; 5: return 0.0 # å‰æœŸä¸ä½¿ç”¨å¯¹æŠ—è®­ç»ƒ elif epoch &lt; total_epochs * 0.7: # çº¿æ€§å¢é•¿é˜¶æ®µ progress = (epoch - 5) / (total_epochs * 0.7 - 5) return progress * self.args.max_alpha else: return self.args.max_alpha # åæœŸä¿æŒæœ€å¤§å€¼ 3. å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥def create_lr_scheduler(self, optimizer, total_steps): \"\"\" åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ ç­–ç•¥ï¼šä½™å¼¦é€€ç« + é¢„çƒ­ 1. é¢„çƒ­é˜¶æ®µï¼šçº¿æ€§å¢é•¿åˆ°åˆå§‹å­¦ä¹ ç‡ 2. ä¸»è®­ç»ƒé˜¶æ®µï¼šä½™å¼¦é€€ç«åˆ°æœ€å°å­¦ä¹ ç‡ 3. é‡å¯æœºåˆ¶ï¼šå‘¨æœŸæ€§é‡å¯æå‡æ€§èƒ½ \"\"\" # é¢„çƒ­è°ƒåº¦å™¨ warmup_scheduler = LinearLR( optimizer, start_factor=0.1, end_factor=1.0, total_iters=self.args.warmup_steps ) # ä½™å¼¦é€€ç«è°ƒåº¦å™¨ cosine_scheduler = CosineAnnealingWarmRestarts( optimizer, T_0=total_steps // 4, # ç¬¬ä¸€ä¸ªå‘¨æœŸé•¿åº¦ T_mult=2, # å‘¨æœŸå€å¢å› å­ eta_min=self.args.min_lr ) # ç»„åˆè°ƒåº¦å™¨ scheduler = SequentialLR( optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[self.args.warmup_steps] ) return scheduler 4. æ•°æ®å¢å¼ºç­–ç•¥def apply_augmentation(self, audio_features): \"\"\" è®­ç»ƒæ—¶æ•°æ®å¢å¼º ç­–ç•¥ï¼š 1. é«˜æ–¯å™ªå£°ï¼šå¢åŠ é²æ£’æ€§ 2. æ—¶é—´æ‹‰ä¼¸ï¼šæ¨¡æ‹Ÿè¯­é€Ÿå˜åŒ– 3. ç‰¹å¾æ©è”½ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ \"\"\" if self.is_training: # 1. æ·»åŠ é«˜æ–¯å™ªå£° if torch.rand(1) &lt; 0.3: noise = torch.randn_like(audio_features) * self.noise_factor audio_features = audio_features + noise # 2. æ—¶é—´ç»´åº¦æ‹‰ä¼¸ï¼ˆç®€åŒ–ç‰ˆï¼‰ if torch.rand(1) &lt; 0.2: stretch_factor = 1.0 + torch.rand(1) * self.time_stretch_factor * 2 - self.time_stretch_factor # å®é™…å®ç°éœ€è¦æ’å€¼æ“ä½œ # 3. ç‰¹å¾æ©è”½ if torch.rand(1) &lt; 0.2: mask_size = int(audio_features.size(0) * 0.1) mask_start = torch.randint(0, max(1, audio_features.size(0) - mask_size), (1,)) audio_features[mask_start:mask_start + mask_size] *= 0.1 return audio_features ğŸ“Š æ€§èƒ½ç›‘æ§ä¸å¯è§†åŒ–1. è®­ç»ƒç›‘æ§æŒ‡æ ‡class TrainingMonitor: \"\"\"è®­ç»ƒè¿‡ç¨‹ç›‘æ§å™¨\"\"\" def __init__(self): self.metrics = { 'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'train_f1': [], 'val_f1': [], 'learning_rate': [], 'alpha_values': [] } def update_metrics(self, epoch, train_metrics, val_metrics, lr, alpha): \"\"\"æ›´æ–°è®­ç»ƒæŒ‡æ ‡\"\"\" self.metrics['train_loss'].append(train_metrics['loss']) self.metrics['val_loss'].append(val_metrics['loss']) self.metrics['train_acc'].append(train_metrics['accuracy']) self.metrics['val_acc'].append(val_metrics['accuracy']) self.metrics['train_f1'].append(train_metrics['f1_score']) self.metrics['val_f1'].append(val_metrics['f1_score']) self.metrics['learning_rate'].append(lr) self.metrics['alpha_values'].append(alpha) def plot_training_curves(self, save_path): \"\"\"ç»˜åˆ¶è®­ç»ƒæ›²çº¿\"\"\" fig, axes = plt.subplots(2, 3, figsize=(18, 12)) # æŸå¤±æ›²çº¿ axes[0, 0].plot(self.metrics['train_loss'], label='Train Loss', color='blue') axes[0, 0].plot(self.metrics['val_loss'], label='Val Loss', color='red') axes[0, 0].set_title('Loss Curves') axes[0, 0].legend() # å‡†ç¡®ç‡æ›²çº¿ axes[0, 1].plot(self.metrics['train_acc'], label='Train Acc', color='blue') axes[0, 1].plot(self.metrics['val_acc'], label='Val Acc', color='red') axes[0, 1].set_title('Accuracy Curves') axes[0, 1].legend() # F1åˆ†æ•°æ›²çº¿ axes[0, 2].plot(self.metrics['train_f1'], label='Train F1', color='blue') axes[0, 2].plot(self.metrics['val_f1'], label='Val F1', color='red') axes[0, 2].set_title('F1 Score Curves') axes[0, 2].legend() # å­¦ä¹ ç‡å˜åŒ– axes[1, 0].plot(self.metrics['learning_rate'], color='green') axes[1, 0].set_title('Learning Rate Schedule') axes[1, 0].set_yscale('log') # Alphaå€¼å˜åŒ– axes[1, 1].plot(self.metrics['alpha_values'], color='orange') axes[1, 1].set_title('Adversarial Alpha Schedule') # éªŒè¯æŸå¤±æ”¾å¤§å›¾ axes[1, 2].plot(self.metrics['val_loss'], color='red', linewidth=2) axes[1, 2].set_title('Validation Loss (Detailed)') plt.tight_layout() plt.savefig(save_path, dpi=300, bbox_inches='tight') plt.close() 2. è·¨è¯´è¯äººæ€§èƒ½åˆ†ædef analyze_speaker_performance(self, model, test_loader, save_dir): \"\"\" è·¨è¯´è¯äººæ€§èƒ½åˆ†æ åˆ†æå†…å®¹ï¼š 1. å„è¯´è¯äººå‡†ç¡®ç‡å¯¹æ¯” 2. æ€§èƒ½æ–¹å·®åˆ†æ 3. æ€§åˆ«å·®å¼‚åˆ†æ 4. ä¼šè¯å·®å¼‚åˆ†æ \"\"\" model.eval() speaker_results = defaultdict(list) with torch.no_grad(): for batch in test_loader: features = batch['audio_features'].to(self.device) labels = batch['emotion_label'].to(self.device) speakers = batch['speaker'] outputs = model(features) predictions = torch.argmax(outputs['emotion_logits'], dim=1) for pred, label, speaker in zip(predictions.cpu(), labels.cpu(), speakers): speaker_results[speaker].append({ 'prediction': pred.item(), 'label': label.item(), 'correct': pred.item() == label.item() }) # è®¡ç®—å„è¯´è¯äººæ€§èƒ½ speaker_metrics = {} for speaker, results in speaker_results.items(): correct = sum(r['correct'] for r in results) total = len(results) accuracy = correct / total if total > 0 else 0 # è®¡ç®—F1åˆ†æ•° y_true = [r['label'] for r in results] y_pred = [r['prediction'] for r in results] f1 = f1_score(y_true, y_pred, average='weighted') speaker_metrics[speaker] = { 'accuracy': accuracy, 'f1_score': f1, 'total_samples': total } # å¯è§†åŒ–ç»“æœ self.plot_speaker_performance(speaker_metrics, save_dir) return speaker_metrics def plot_speaker_performance(self, speaker_metrics, save_dir): \"\"\"ç»˜åˆ¶è¯´è¯äººæ€§èƒ½å¯¹æ¯”å›¾\"\"\" speakers = list(speaker_metrics.keys()) accuracies = [speaker_metrics[s]['accuracy'] for s in speakers] f1_scores = [speaker_metrics[s]['f1_score'] for s in speakers] fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6)) # å‡†ç¡®ç‡å¯¹æ¯” bars1 = ax1.bar(speakers, accuracies, color='skyblue', alpha=0.7) ax1.set_title('Speaker-wise Accuracy Comparison') ax1.set_ylabel('Accuracy') ax1.set_ylim(0, 1) # æ·»åŠ æ•°å€¼æ ‡ç­¾ for bar, acc in zip(bars1, accuracies): ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{acc:.3f}', ha='center', va='bottom') # F1åˆ†æ•°å¯¹æ¯” bars2 = ax2.bar(speakers, f1_scores, color='lightcoral', alpha=0.7) ax2.set_title('Speaker-wise F1 Score Comparison') ax2.set_ylabel('F1 Score') ax2.set_ylim(0, 1) # æ·»åŠ æ•°å€¼æ ‡ç­¾ for bar, f1 in zip(bars2, f1_scores): ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{f1:.3f}', ha='center', va='bottom') plt.tight_layout() plt.savefig(f'{save_dir}/speaker_performance_comparison.png', dpi=300, bbox_inches='tight') plt.close() ğŸ¯ ä½¿ç”¨ç¤ºä¾‹1. æ¨¡å‹åˆå§‹åŒ–# åˆ›å»ºå‚æ•°å¯¹è±¡ args = argparse.Namespace( input_size=768, hidden_size=256, output_size=4, dia_layers=3, dropout=0.3, attention=True, speaker_norm=True, speaker_adversarial=True, adversarial_weight=0.05, max_alpha=1.0 ) # åˆå§‹åŒ–æ¨¡å‹ model = EnhancedGRUModel( input_size=args.input_size, hidden_size=args.hidden_size, output_size=args.output_size, args=args ) print(f\"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\") 2. è®­ç»ƒæµç¨‹# åˆ›å»ºè®­ç»ƒå™¨ trainer = AdvancedTrainer(args) # è®­ç»ƒæ¨¡å‹ best_model_path = trainer.train_model( model=model, train_loader=train_loader, val_loader=val_loader, save_dir='./experiments' ) print(f\"æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {best_model_path}\") 3. æ€§èƒ½è¯„ä¼°# åŠ è½½æœ€ä½³æ¨¡å‹ model.load_state_dict(torch.load(best_model_path)) # è¯„ä¼°è·¨è¯´è¯äººæ€§èƒ½ evaluator = SpeakerIndependenceEvaluator(model, args) results = evaluator.evaluate(test_loader, save_dir='./evaluation_results') print(\"è·¨è¯´è¯äººæ€§èƒ½è¯„ä¼°å®Œæˆï¼\") print(f\"æ€»ä½“å‡†ç¡®ç‡: {results['overall_accuracy']:.4f}\") print(f\"å¹³å‡F1åˆ†æ•°: {results['average_f1']:.4f}\") print(f\"æ€§èƒ½æ ‡å‡†å·®: {results['performance_std']:.4f}\") ğŸ“ˆ é¢„æœŸæ”¹è¿›æ•ˆæœæ€§èƒ½æå‡é¢„æœŸ æŒ‡æ ‡ åŸå§‹æ¨¡å‹ å¢å¼ºæ¨¡å‹ æ”¹è¿›å¹…åº¦ æ€»ä½“å‡†ç¡®ç‡ 65-70% 75-80% +10-15% è·¨è¯´è¯äººF1 0.62-0.67 0.72-0.77 +0.10-0.15 æ€§èƒ½æ–¹å·® 0.08-0.12 0.04-0.08 -50%â†“ æ”¶æ•›é€Ÿåº¦ 30-40è½® 20-25è½® å¿«25-50% æŠ€æœ¯ä¼˜åŠ¿ ğŸ¯ è¯´è¯äººæ— å…³æ€§ï¼šAdaINå½’ä¸€åŒ– + å¯¹æŠ—è®­ç»ƒæ˜¾è‘—å‡å°‘è¯´è¯äººåè§ ğŸš€ è®­ç»ƒæ•ˆç‡ï¼šåŠ¨æ€å­¦ä¹ ç‡è°ƒåº¦ + æ—©åœæœºåˆ¶åŠ é€Ÿæ”¶æ•› ğŸ’ª æ¨¡å‹é²æ£’æ€§ï¼šå¤šç§æ­£åˆ™åŒ–æŠ€æœ¯æå‡æ³›åŒ–èƒ½åŠ› ğŸ“Š å…¨é¢ç›‘æ§ï¼šå®æ—¶å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹å’Œæ€§èƒ½æŒ‡æ ‡ ğŸ”§ æ•…éšœæ’é™¤å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ å†…å­˜ä¸è¶³ # å‡å°‘æ‰¹æ¬¡å¤§å° args.batch_size = 16 # ä»32é™åˆ°16 # å¯ç”¨æ¢¯åº¦ç´¯ç§¯ args.gradient_accumulation_steps = 2 è®­ç»ƒä¸ç¨³å®š # é™ä½å­¦ä¹ ç‡ args.learning_rate = 0.0001 # å¢åŠ æ¢¯åº¦è£å‰ª args.max_grad_norm = 0.5 è¿‡æ‹Ÿåˆä¸¥é‡ # å¢åŠ Dropout args.dropout = 0.5 # å¢åŠ L2æ­£åˆ™åŒ– args.l2_reg = 1e-4 å¯¹æŠ—è®­ç»ƒä¸æ”¶æ•› # é™ä½å¯¹æŠ—æƒé‡ args.adversarial_weight = 0.01 # å»¶è¿Ÿå¯¹æŠ—è®­ç»ƒå¼€å§‹æ—¶é—´ args.adversarial_start_epoch = 10 ğŸ“š å‚è€ƒæ–‡çŒ® AdaIN: Huang, X., &amp; Belongie, S. (2017). Arbitrary style transfer in real-time with adaptive instance normalization. Gradient Reversal: Ganin, Y., &amp; Lempitsky, V. (2015). Unsupervised domain adaptation by backpropagation. Multi-Head Attention: Vaswani, A., et al. (2017). Attention is all you need. HuBERT: Hsu, W. N., et al. (2021). HuBERT: Self-supervised speech representation learning by masked prediction. ğŸ“ æ–‡æ¡£ç‰ˆæœ¬: v2.0 | æ›´æ–°æ—¥æœŸ: 2024-09-26 | ä½œè€…: AI Assistant IEMOCAPè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿæ·±åº¦æºç è§£æç›®å½• é¡¹ç›®æ•´ä½“æ¶æ„ä¸æ¨¡å—åˆ’åˆ† æ ¸å¿ƒç»„ä»¶åŠŸèƒ½æ·±åº¦è§£æ å®Œæ•´æ•°æ®æµè·¯å¾„åˆ†æ å…³é”®å‚æ•°å«ä¹‰ä¸æ€§èƒ½å½±å“ æ¨¡å‹å·¥ä½œæœºåˆ¶æ·±å…¥ç†è§£ ç³»ç»Ÿä¼˜åŠ¿ä¸æŠ€æœ¯åˆ›æ–° 1. é¡¹ç›®æ•´ä½“æ¶æ„ä¸æ¨¡å—åˆ’åˆ†1.1 ç³»ç»Ÿæ¶æ„æ¦‚è§ˆè¯¥IEMOCAPè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿé‡‡ç”¨ç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå®ç°ä»åŸå§‹éŸ³é¢‘ä¿¡å·åˆ°æƒ…æ„Ÿç±»åˆ«çš„ç›´æ¥æ˜ å°„ã€‚æ•´ä½“æ•°æ®æµéµå¾ªç°ä»£è¯­éŸ³å¤„ç†çš„æœ€ä½³å®è·µï¼š åŸå§‹éŸ³é¢‘ â†’ é¢„å¤„ç†æ ‡å‡†åŒ– â†’ HuBERTç‰¹å¾ç¼–ç  â†’ åŒå‘GRUåºåˆ—å»ºæ¨¡ â†’ æ³¨æ„åŠ›æœºåˆ¶å¢å¼º â†’ å…¨å±€æ± åŒ– â†’ åˆ†ç±»è¾“å‡º è¿™ç§è®¾è®¡å……åˆ†åˆ©ç”¨äº†è‡ªç›‘ç£é¢„è®­ç»ƒæ¨¡å‹çš„å¼ºå¤§ç‰¹å¾æå–èƒ½åŠ›ï¼Œç»“åˆå¾ªç¯ç¥ç»ç½‘ç»œå¯¹æ—¶åºä¿¡æ¯çš„ç²¾ç¡®å»ºæ¨¡ï¼Œæœ€ç»ˆé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å®ç°å¯¹æƒ…æ„Ÿå…³é”®ä¿¡æ¯çš„åŠ¨æ€èšç„¦ã€‚ 1.2 æ ¸å¿ƒæ¨¡å—åˆ’åˆ†æ•°æ®é¢„å¤„ç†æ¨¡å— (Data_prepocessing.py) åŠŸèƒ½èŒè´£ï¼šè´Ÿè´£IEMOCAPæ•°æ®é›†çš„æ ‡å‡†åŒ–å¤„ç†ï¼ŒåŒ…æ‹¬éŸ³é¢‘é•¿åº¦ç»Ÿä¸€ã€é‡‡æ ·ç‡æ ‡å‡†åŒ–ã€æƒ…æ„Ÿæ ‡ç­¾æ˜ å°„ æ ¸å¿ƒä»·å€¼ï¼šç¡®ä¿æ¨¡å‹è¾“å…¥çš„ä¸€è‡´æ€§ï¼Œä¸ºåç»­ç‰¹å¾æå–æä¾›æ ‡å‡†åŒ–çš„æ•°æ®åŸºç¡€ æŠ€æœ¯ç‰¹ç‚¹ï¼šé‡‡ç”¨å›ºå®š3ç§’æ—¶é•¿ç­–ç•¥ï¼Œå¹³è¡¡ä¿¡æ¯ä¿ç•™ä¸è®¡ç®—æ•ˆç‡ æ¨¡å‹æ¶æ„æ¨¡å— (models/GRU.py) SpeechRecognitionModelï¼šä¸»æ¨¡å‹å®¹å™¨ï¼Œæ•´åˆHuBERTç‰¹å¾æå–å™¨ä¸GRUåºåˆ—å»ºæ¨¡å™¨ GRUModelï¼šåºåˆ—å»ºæ¨¡æ ¸å¿ƒï¼Œè´Ÿè´£æ—¶åºç‰¹å¾çš„æ·±åº¦å­¦ä¹ ä¸æƒ…æ„Ÿåˆ†ç±» MatchingAttentionï¼šæ³¨æ„åŠ›æœºåˆ¶å®ç°ï¼Œæä¾›åŠ¨æ€ç‰¹å¾åŠ æƒèƒ½åŠ› è®­ç»ƒä¸éªŒè¯æ¨¡å— (train.py) äº¤å‰éªŒè¯ç­–ç•¥ï¼šé‡‡ç”¨5æŠ˜äº¤å‰éªŒè¯ï¼Œç¡®ä¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„å¯é è¯„ä¼° ä¼˜åŒ–ç­–ç•¥ï¼šä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œç»“åˆé€‚å½“çš„å­¦ä¹ ç‡è°ƒåº¦ æ€§èƒ½è¯„ä¼°ï¼šå¤šæŒ‡æ ‡ç»¼åˆè¯„ä¼°ï¼ŒåŒ…æ‹¬å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•° æ¨ç†ä¸åº”ç”¨æ¨¡å— (DEMO.py, GUIæƒ…æ„Ÿè¯†åˆ«2.py) å•æ ·æœ¬æ¨ç†ï¼šæä¾›ç®€æ´çš„æ¨¡å‹æµ‹è¯•æ¥å£ å®æ—¶éŸ³é¢‘å¤„ç†ï¼šæ”¯æŒéº¦å…‹é£å®æ—¶å½•éŸ³ä¸æƒ…æ„Ÿè¯†åˆ« ç”¨æˆ·ç•Œé¢ï¼šå®Œæ•´çš„PyQt5å›¾å½¢ç•Œé¢ï¼Œæä¾›ç›´è§‚çš„äº¤äº’ä½“éªŒ 2. æ ¸å¿ƒç»„ä»¶åŠŸèƒ½æ·±åº¦è§£æ2.1 HubertModelè¯­éŸ³ç‰¹å¾ç¼–ç å™¨2.1.1 æ¨¡å‹é€‰æ‹©çš„æ·±å±‚è€ƒé‡self.feature_extractor = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\") HuBERT (Hidden-Unit BERT) çš„é€‰æ‹©ä½“ç°äº†å¯¹è¯­éŸ³è¡¨ç¤ºå­¦ä¹ å‰æ²¿æŠ€æœ¯çš„æ·±åˆ»ç†è§£ï¼š è‡ªç›‘ç£å­¦ä¹ ä¼˜åŠ¿ï¼š HuBERTé€šè¿‡æ©ç é¢„æµ‹ä»»åŠ¡åœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨è¯­éŸ³æ•°æ®ä¸Šé¢„è®­ç»ƒï¼Œå­¦ä¹ åˆ°äº†ä¸°å¯Œçš„è¯­éŸ³è¡¨ç¤º ç›¸æ¯”ä¼ ç»Ÿçš„MFCCã€Melé¢‘è°±ç­‰æ‰‹å·¥ç‰¹å¾ï¼ŒHuBERTèƒ½å¤Ÿè‡ªåŠ¨å‘ç°è¯­éŸ³ä¸­çš„å±‚æ¬¡åŒ–æ¨¡å¼ é¢„è®­ç»ƒåœ¨960å°æ—¶LibriSpeechæ•°æ®ä¸Šè¿›è¡Œï¼Œæ¶µç›–äº†å¤šæ ·åŒ–çš„è¯­éŸ³æ¨¡å¼å’Œå£°å­¦ç¯å¢ƒ åˆ†å±‚ç‰¹å¾è¡¨ç¤ºï¼š åº•å±‚ï¼šæ•è·éŸ³ç´ çº§åˆ«çš„å£°å­¦ç‰¹å¾ï¼Œå¦‚å…±æŒ¯å³°ã€åŸºé¢‘å˜åŒ– ä¸­å±‚ï¼šå»ºæ¨¡éŸ³èŠ‚å’Œè¯æ±‡çº§åˆ«çš„è¯­éŸ³æ¨¡å¼ é«˜å±‚ï¼šç¼–ç è¯­ä¹‰å’ŒéŸµå¾‹ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯å¯¹æƒ…æ„Ÿè¯†åˆ«è‡³å…³é‡è¦ 768ç»´ç‰¹å¾å‘é‡çš„ä¿¡æ¯å¯†åº¦ï¼š æ¯ä¸ªæ—¶é—´æ­¥è¾“å‡º768ç»´å¯†é›†å‘é‡ï¼Œç›¸æ¯”ä¼ ç»Ÿç‰¹å¾ï¼ˆå¦‚39ç»´MFCCï¼‰å…·æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ› é«˜ç»´ç‰¹å¾ç©ºé—´èƒ½å¤Ÿæ›´ç²¾ç»†åœ°åŒºåˆ†ä¸åŒæƒ…æ„ŸçŠ¶æ€ä¸‹çš„è¯­éŸ³å˜åŒ– 2.1.2 ç‰¹å¾æå–çš„æŠ€æœ¯å®ç°def forward(self, input_waveform): features = self.feature_extractor(input_waveform).last_hidden_state # [batch, seq_len, 768] logits = self.Utterance_net(features) return logits, features å¤„ç†æµç¨‹çš„æŠ€æœ¯ç»†èŠ‚ï¼š å·ç§¯ç‰¹å¾æå–ï¼š HuBERTé¦–å…ˆé€šè¿‡7å±‚1Då·ç§¯ç½‘ç»œå¤„ç†åŸå§‹æ³¢å½¢ æ¯å±‚å·ç§¯é€æ­¥é™ä½æ—¶é—´åˆ†è¾¨ç‡ï¼Œæé«˜ç‰¹å¾æŠ½è±¡å±‚æ¬¡ å·ç§¯æ ¸è®¾è®¡è€ƒè™‘äº†è¯­éŸ³ä¿¡å·çš„æ—¶é¢‘ç‰¹æ€§ Transformerç¼–ç ï¼š 12å±‚Transformerç¼–ç å™¨è¿›è¡Œåºåˆ—å»ºæ¨¡ è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•è·é•¿è·ç¦»ä¾èµ–å…³ç³» ä½ç½®ç¼–ç ä¿æŒæ—¶åºä¿¡æ¯çš„å®Œæ•´æ€§ ç‰¹å¾é€‰æ‹©ç­–ç•¥ï¼š last_hidden_stateæä¾›æœ€é«˜å±‚çš„è¯­ä¹‰è¡¨ç¤º è¿™ä¸€å±‚ç‰¹å¾æœ€é€‚åˆä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ï¼Œå¹³è¡¡äº†ç‰¹å¾æŠ½è±¡ç¨‹åº¦ä¸ä»»åŠ¡ç›¸å…³æ€§ 2.1.3 éŸ³é¢‘é¢„å¤„ç†çš„å·¥ç¨‹è€ƒé‡def process_wav_file(wav_file, time_seconds): waveform, sample_rate = torchaudio.load(wav_file) target_length = int(time_seconds * sample_rate) if waveform.size(1) > target_length: waveform = waveform[:, :target_length] # æ—¶é—´è£å‰ª else: padding_length = target_length - waveform.size(1) waveform = torch.nn.functional.pad(waveform, (0, padding_length)) # é›¶å¡«å…… return waveform, sample_rate 3ç§’å›ºå®šé•¿åº¦çš„è®¾è®¡rationaleï¼š è®¡ç®—æ•ˆç‡ï¼šå›ºå®šé•¿åº¦ä¾¿äºæ‰¹å¤„ç†ï¼Œæé«˜GPUåˆ©ç”¨ç‡ ä¿¡æ¯å……åˆ†æ€§ï¼š3ç§’è¶³ä»¥åŒ…å«å®Œæ•´çš„æƒ…æ„Ÿè¡¨è¾¾ï¼Œæ¶µç›–è¯æ±‡ã€éŸµå¾‹ã€è¯­è°ƒå˜åŒ– å†…å­˜ç®¡ç†ï¼šé¿å…å˜é•¿åºåˆ—å¸¦æ¥çš„å†…å­˜ç¢ç‰‡åŒ–é—®é¢˜ æ¨¡å‹ä¸€è‡´æ€§ï¼šç¡®ä¿è®­ç»ƒå’Œæ¨ç†é˜¶æ®µçš„è¾“å…¥æ ¼å¼å®Œå…¨ä¸€è‡´ é›¶å¡«å…… vs é‡å¤å¡«å……çš„é€‰æ‹©ï¼š é›¶å¡«å……é¿å…äº†äººå·¥å¼•å…¥çš„å‘¨æœŸæ€§æ¨¡å¼ ä¿æŒäº†åŸå§‹è¯­éŸ³çš„è‡ªç„¶è¾¹ç•Œç‰¹æ€§ ä¸HuBERTé¢„è®­ç»ƒæ—¶çš„å¤„ç†æ–¹å¼ä¿æŒä¸€è‡´ 2.2 GRUModelåŒå‘åºåˆ—å»ºæ¨¡å™¨2.2.1 æ¶æ„è®¾è®¡çš„æ·±å±‚é€»è¾‘class GRUModel(nn.Module): def __init__(self, input_size, hidden_size, output_size, args): self.bigru = nn.GRU(input_size, hidden_size, batch_first=True, num_layers=self.num_layers, bidirectional=True) self.input2hidden = nn.Linear(512, hidden_size * 2) self.hidden2label = nn.Linear(hidden_size * 2, output_size) åŒå‘GRUçš„ç†è®ºåŸºç¡€ï¼š å‰å‘ä¿¡æ¯æµï¼šæ•è·ä»è¯­éŸ³å¼€å§‹åˆ°å½“å‰æ—¶åˆ»çš„æƒ…æ„Ÿå‘å±•è½¨è¿¹ åå‘ä¿¡æ¯æµï¼šåˆ©ç”¨æœªæ¥ä¿¡æ¯ä¸ºå½“å‰æ—¶åˆ»æä¾›ä¸Šä¸‹æ–‡çº¦æŸ ä¿¡æ¯èåˆï¼šå‰åå‘éšçŠ¶æ€çš„æ‹¼æ¥æä¾›äº†æ›´å®Œæ•´çš„æ—¶åºè¡¨ç¤º å¤šå±‚è®¾è®¡çš„å¿…è¦æ€§ï¼š å±‚æ¬¡åŒ–æŠ½è±¡ï¼šåº•å±‚æ•è·å±€éƒ¨æ—¶åºæ¨¡å¼ï¼Œé«˜å±‚å»ºæ¨¡å…¨å±€æƒ…æ„ŸåŠ¨æ€ éçº¿æ€§å¢å¼ºï¼šå¤šå±‚ç»“æ„å¢åŠ æ¨¡å‹çš„éçº¿æ€§è¡¨è¾¾èƒ½åŠ› æ¢¯åº¦æµä¼˜åŒ–ï¼šé€‚å½“çš„å±‚æ•°å¹³è¡¡äº†è¡¨è¾¾èƒ½åŠ›ä¸æ¢¯åº¦ä¼ æ’­æ•ˆç‡ 2.2.2 å‰å‘ä¼ æ’­çš„ç²¾å¯†è®¾è®¡def forward(self, U): U = self.dropout(U) # è¾“å…¥æ­£åˆ™åŒ– emotions, hidden = self.bigru(U) # [batch, seq, 512] # æ³¨æ„åŠ›æœºåˆ¶å¢å¼º if self.attention: att_emotions = [] for t in emotions: att_em, alpha_ = self.matchatt(emotions, t, mask=None) att_emotions.append(att_em.unsqueeze(0)) att_emotions = torch.cat(att_emotions, dim=0) emotions = att_emotions # å…¨å±€ç‰¹å¾èšåˆ gru_out = torch.transpose(emotions, 1, 2) # [batch, 512, seq] gru_out = F.tanh(gru_out) gru_out = F.max_pool1d(gru_out, gru_out.size(2)).squeeze(2) # å…¨å±€æœ€å¤§æ± åŒ– # åˆ†ç±»æ˜ å°„ Out_in = self.relu(gru_out) Out_in = self.dropout(Out_in) Out_out = self.hidden2label(Out_in) # [batch, num_classes] return Out_out å…³é”®å¤„ç†æ­¥éª¤çš„æŠ€æœ¯åˆ†æï¼š è¾“å…¥Dropoutï¼š åœ¨ç‰¹å¾å±‚é¢å¼•å…¥éšæœºæ€§ï¼Œå¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ› é˜²æ­¢æ¨¡å‹è¿‡åº¦ä¾èµ–HuBERTç‰¹å¾çš„ç‰¹å®šç»´åº¦ åŒå‘GRUå¤„ç†ï¼š è¾“å‡ºç»´åº¦ä¸º512ï¼ˆ256Ã—2ï¼‰ï¼Œèåˆå‰åå‘ä¿¡æ¯ batch_first=Trueè®¾è®¡ä¾¿äºåç»­å¤„ç†å’Œè°ƒè¯• æ³¨æ„åŠ›å¢å¼ºï¼ˆå¯é€‰ï¼‰ï¼š ä¸ºæ¯ä¸ªæ—¶é—´æ­¥è®¡ç®—å…¨å±€æ³¨æ„åŠ›æƒé‡ åŠ¨æ€è°ƒæ•´ä¸åŒæ—¶åˆ»ç‰¹å¾çš„é‡è¦æ€§ ç¼“è§£é•¿åºåˆ—ä¿¡æ¯è¡°å‡é—®é¢˜ å…¨å±€æœ€å¤§æ± åŒ–ï¼š æå–åºåˆ—ä¸­çš„æœ€æ˜¾è‘—ç‰¹å¾ å®ç°ä»å˜é•¿åºåˆ—åˆ°å›ºå®šé•¿åº¦è¡¨ç¤ºçš„è½¬æ¢ ä¿ç•™æœ€å¼ºçš„æƒ…æ„Ÿæ¿€æ´»ä¿¡å· åˆ†ç±»å¤´æ˜ å°„ï¼š çº¿æ€§å˜æ¢å°†512ç»´ç‰¹å¾æ˜ å°„åˆ°4ç±»æƒ…æ„Ÿè¾“å‡º æ— åç½®è®¾è®¡ç®€åŒ–æ¨¡å‹ï¼Œå‡å°‘è¿‡æ‹Ÿåˆé£é™© 2.3 MatchingAttentionæ³¨æ„åŠ›æœºåˆ¶2.3.1 æ³¨æ„åŠ›è®¾è®¡çš„ç†è®ºåŸºç¡€class MatchingAttention(nn.Module): def __init__(self, mem_dim, cand_dim, alpha_dim=None, att_type='general'): if att_type=='general': self.transform = nn.Linear(cand_dim, mem_dim, bias=False) æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³ï¼š æŸ¥è¯¢-é”®-å€¼æ¨¡å¼ï¼šå°†å½“å‰æ—¶åˆ»ä½œä¸ºæŸ¥è¯¢ï¼Œæ•´ä¸ªåºåˆ—ä½œä¸ºé”®å’Œå€¼ ç›¸ä¼¼åº¦è®¡ç®—ï¼šé€šè¿‡å­¦ä¹ åˆ°çš„å˜æ¢çŸ©é˜µè®¡ç®—æŸ¥è¯¢ä¸é”®çš„åŒ¹é…ç¨‹åº¦ åŠ¨æ€æƒé‡åˆ†é…ï¼šæ ¹æ®ç›¸ä¼¼åº¦ä¸ºä¸åŒæ—¶åˆ»åˆ†é…æ³¨æ„åŠ›æƒé‡ General Attentionçš„ä¼˜åŠ¿ï¼š ç»´åº¦çµæ´»æ€§ï¼šé€šè¿‡çº¿æ€§å˜æ¢å¤„ç†ä¸åŒç»´åº¦çš„è¾“å…¥ å‚æ•°æ•ˆç‡ï¼šç›¸æ¯”concat attentionå‚æ•°æ›´å°‘ï¼Œè®­ç»ƒæ›´ç¨³å®š è®¡ç®—æ•ˆç‡ï¼šçŸ©é˜µä¹˜æ³•æ“ä½œä¾¿äºGPUå¹¶è¡ŒåŠ é€Ÿ 2.3.2 æ³¨æ„åŠ›è®¡ç®—çš„æ•°å­¦å®ç°def forward(self, M, x, mask=None): # M: [seq_len, batch, mem_dim] - è®°å¿†åºåˆ—ï¼ˆæ‰€æœ‰æ—¶åˆ»çš„éšçŠ¶æ€ï¼‰ # x: [batch, cand_dim] - æŸ¥è¯¢å‘é‡ï¼ˆå½“å‰æ—¶åˆ»çš„éšçŠ¶æ€ï¼‰ if self.att_type=='general': M_ = M.permute(1,2,0) # [batch, mem_dim, seq_len] x_ = self.transform(x).unsqueeze(1) # [batch, 1, mem_dim] alpha = F.softmax(torch.bmm(x_, M_), dim=2) # [batch, 1, seq_len] attn_pool = torch.bmm(alpha, M.transpose(0,1))[:,0,:] # [batch, mem_dim] return attn_pool, alpha è®¡ç®—æµç¨‹çš„æ·±å±‚è§£æï¼š æŸ¥è¯¢å˜æ¢ï¼š x_ = self.transform(x).unsqueeze(1) # [batch, 1, mem_dim] å°†å½“å‰æ—¶åˆ»ç‰¹å¾å˜æ¢åˆ°è®°å¿†ç©ºé—´ å­¦ä¹ æŸ¥è¯¢ä¸è®°å¿†ä¹‹é—´çš„æœ€ä¼˜åŒ¹é…å…³ç³» ç›¸ä¼¼åº¦è®¡ç®—ï¼š alpha = F.softmax(torch.bmm(x_, M_), dim=2) # [batch, 1, seq_len] æ‰¹é‡çŸ©é˜µä¹˜æ³•è®¡ç®—æ‰€æœ‰æ—¶åˆ»çš„ç›¸ä¼¼åº¦åˆ†æ•° Softmaxå½’ä¸€åŒ–ç¡®ä¿æƒé‡å’Œä¸º1 åŠ æƒèšåˆï¼š attn_pool = torch.bmm(alpha, M.transpose(0,1))[:,0,:] # [batch, mem_dim] æ ¹æ®æ³¨æ„åŠ›æƒé‡å¯¹æ‰€æœ‰æ—¶åˆ»ç‰¹å¾è¿›è¡ŒåŠ æƒå¹³å‡ ç”Ÿæˆèåˆå…¨å±€ä¿¡æ¯çš„ä¸Šä¸‹æ–‡å‘é‡ 2.3.3 æ³¨æ„åŠ›åœ¨æƒ…æ„Ÿè¯†åˆ«ä¸­çš„ä½œç”¨æœºåˆ¶if self.attention: att_emotions = [] alpha = [] for t in emotions: # å¯¹åºåˆ—ä¸­æ¯ä¸ªæ—¶é—´æ­¥ att_em, alpha_ = self.matchatt(emotions, t, mask=None) att_emotions.append(att_em.unsqueeze(0)) alpha.append(alpha_[:, 0, :]) att_emotions = torch.cat(att_emotions, dim=0) emotions = att_emotions æ³¨æ„åŠ›å¢å¼ºçš„æƒ…æ„Ÿå»ºæ¨¡ä»·å€¼ï¼š å…³é”®æ—¶åˆ»è¯†åˆ«ï¼š è‡ªåŠ¨è¯†åˆ«è¯­éŸ³ä¸­æƒ…æ„Ÿè¡¨è¾¾æœ€å¼ºçƒˆçš„æ—¶é—´æ®µ ä¾‹å¦‚ï¼šè¯­è°ƒå˜åŒ–å‰§çƒˆçš„è¯æ±‡ã€åœé¡¿å‰åçš„é‡éŸ³ ä¸Šä¸‹æ–‡æ•´åˆï¼š ä¸ºæ¯ä¸ªæ—¶åˆ»æä¾›å…¨åºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ é¿å…å±€éƒ¨ç‰¹å¾çš„è¯¯å¯¼ï¼Œæé«˜åˆ†ç±»ç¨³å®šæ€§ é•¿è·ç¦»ä¾èµ–å»ºæ¨¡ï¼š ç¼“è§£GRUåœ¨é•¿åºåˆ—ä¸Šçš„ä¿¡æ¯è¡°å‡é—®é¢˜ ä¿æŒåºåˆ—å¼€å§‹å’Œç»“æŸéƒ¨åˆ†ä¿¡æ¯çš„æœ‰æ•ˆä¼ é€’ å¯è§£é‡Šæ€§å¢å¼ºï¼š æ³¨æ„åŠ›æƒé‡æä¾›æ¨¡å‹å†³ç­–çš„å¯è§†åŒ–ä¾æ® å¸®åŠ©ç†è§£æ¨¡å‹å…³æ³¨çš„è¯­éŸ³ç‰¹å¾æ¨¡å¼ 2.4 åˆ†ç±»å¤´ä¸æ¿€æ´»å‡½æ•°çš„ç²¾å¿ƒè®¾è®¡2.4.1 åˆ†ç±»å¤´çš„æ¶æ„é€‰æ‹©self.hidden2label = nn.Linear(hidden_size * 2, output_size) # 512 -> 4 çº¿æ€§åˆ†ç±»å¤´çš„è®¾è®¡è€ƒé‡ï¼š ç®€æ´æ€§åŸåˆ™ï¼šé¿å…è¿‡åº¦å¤æ‚çš„åˆ†ç±»å™¨ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ ç‰¹å¾å……åˆ†æ€§ï¼š512ç»´GRUè¾“å‡ºå·²åŒ…å«ä¸°å¯Œçš„æƒ…æ„Ÿåˆ¤åˆ«ä¿¡æ¯ è®¡ç®—æ•ˆç‡ï¼šçº¿æ€§å˜æ¢è®¡ç®—ç®€å•ï¼Œä¾¿äºå®æ—¶åº”ç”¨ 2.4.2 æ¿€æ´»å‡½æ•°çš„å±‚æ¬¡åŒ–åº”ç”¨gru_out = F.tanh(gru_out) # åºåˆ—ç‰¹å¾æ¿€æ´» Out_in = self.relu(gru_out) # åˆ†ç±»å‰æ¿€æ´» # åˆ†ç±»å±‚æ— æ¿€æ´»ï¼Œè¾“å‡ºåŸå§‹logits æ¿€æ´»å‡½æ•°é€‰æ‹©çš„æ·±å±‚é€»è¾‘ï¼š Tanhæ¿€æ´»ï¼š å°†åºåˆ—ç‰¹å¾å‹ç¼©åˆ°[-1,1]åŒºé—´ å¢å¼ºç‰¹å¾çš„å¯¹æ¯”åº¦ï¼Œçªå‡ºæ˜¾è‘—å˜åŒ– å¯¹ç§°æ€§è´¨é€‚åˆåŒå‘GRUçš„è¾“å‡ºç‰¹å¾ LeakyReLUæ¿€æ´»ï¼š ä¿æŒæ¢¯åº¦æµåŠ¨ï¼Œé¿å…æ­»ç¥ç»å…ƒé—®é¢˜ è´Ÿæ–œç‡å‚æ•°å…è®¸è´Ÿå€¼ä¿¡æ¯çš„éƒ¨åˆ†ä¿ç•™ åœ¨åˆ†ç±»å‰æä¾›éçº¿æ€§å˜æ¢èƒ½åŠ› æ— æ¿€æ´»è¾“å‡ºï¼š åˆ†ç±»å±‚è¾“å‡ºåŸå§‹logitsï¼Œä¾¿äºäº¤å‰ç†µæŸå¤±è®¡ç®— ä¿æŒæ•°å€¼ç¨³å®šæ€§ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤± 3. å®Œæ•´æ•°æ®æµè·¯å¾„åˆ†æ3.1 è®­ç»ƒé˜¶æ®µæ•°æ®æµè®­ç»ƒæµç¨‹çš„å…³é”®ç¯èŠ‚åˆ†æï¼š æ•°æ®é¢„å¤„ç†é˜¶æ®µï¼š IEMOCAPæ•°æ®é›†åŒ…å«å¤šç§æƒ…æ„Ÿç±»åˆ«ï¼Œéœ€è¦æ ‡å‡†åŒ–æ˜ å°„ éŸ³é¢‘é•¿åº¦ä¸ä¸€è‡´é—®é¢˜é€šè¿‡3ç§’å›ºå®šé•¿åº¦ç­–ç•¥è§£å†³ é‡‡æ ·ç‡ç»Ÿä¸€ä¸º16kHzï¼ŒåŒ¹é…HuBERTé¢„è®­ç»ƒé…ç½® ç‰¹å¾æå–é˜¶æ®µï¼š HuBERTæ¨¡å‹å†»ç»“å‚æ•°ï¼Œä»…ç”¨äºç‰¹å¾æå– 768ç»´ç‰¹å¾å‘é‡åŒ…å«ä¸°å¯Œçš„è¯­éŸ³è¯­ä¹‰ä¿¡æ¯ æ‰¹å¤„ç†æ–¹å¼æé«˜ç‰¹å¾æå–æ•ˆç‡ æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼š 5æŠ˜äº¤å‰éªŒè¯ç¡®ä¿ç»“æœçš„ç»Ÿè®¡æ˜¾è‘—æ€§ æ‰¹æ¬¡å¤§å°32å¹³è¡¡å†…å­˜å ç”¨ä¸æ¢¯åº¦ä¼°è®¡è´¨é‡ AdamWä¼˜åŒ–å™¨ç»“åˆæƒé‡è¡°å‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ æ¨¡å‹ä¿å­˜é˜¶æ®µï¼š ä¿å­˜å®Œæ•´çš„state_dictï¼Œä¾¿äºåç»­åŠ è½½ æ¨¡å‹æ–‡ä»¶åŒ…å«æ‰€æœ‰å¯è®­ç»ƒå‚æ•° 3.2 æ¨ç†é˜¶æ®µæ•°æ®æµæ¨ç†æµç¨‹çš„æŠ€æœ¯ç»†èŠ‚ï¼š è¾“å…¥å¤„ç†å¤šæ ·æ€§ï¼š æ”¯æŒWAVæ–‡ä»¶å’Œå®æ—¶éº¦å…‹é£ä¸¤ç§è¾“å…¥æ¨¡å¼ ç»Ÿä¸€çš„é¢„å¤„ç†æµç¨‹ç¡®ä¿è¾“å…¥æ ¼å¼ä¸€è‡´æ€§ ç‰¹å¾æå–ä¸€è‡´æ€§ï¼š ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„processorå’Œé¢„å¤„ç†å‚æ•° ç¡®ä¿ç‰¹å¾åˆ†å¸ƒçš„ä¸€è‡´æ€§ æ¨¡å‹æ¨ç†ä¼˜åŒ–ï¼š torch.no_grad()ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡å°‘å†…å­˜å ç”¨ æ‰¹å¤„ç†ç»´åº¦çš„åŠ¨æ€è°ƒæ•´é€‚åº”ä¸åŒè¾“å…¥æ ¼å¼ ç»“æœåå¤„ç†ï¼š Softmaxæä¾›æ¦‚ç‡åˆ†å¸ƒï¼Œå¢å¼ºç»“æœå¯ä¿¡åº¦ ç½®ä¿¡åº¦è®¡ç®—å¸®åŠ©è¯„ä¼°é¢„æµ‹è´¨é‡ 3.3 GUIå®æ—¶å¤„ç†æµå®æ—¶å¤„ç†çš„å·¥ç¨‹æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆï¼š çº¿ç¨‹å®‰å…¨è®¾è®¡ï¼š AudioRecorderç‹¬ç«‹çº¿ç¨‹é¿å…UIé˜»å¡ ä¿¡å·-æ§½æœºåˆ¶ç¡®ä¿çº¿ç¨‹é—´å®‰å…¨é€šä¿¡ éŸ³é¢‘ç¼“å†²ç®¡ç†ï¼š æ»‘åŠ¨çª—å£æœºåˆ¶ä¿æŒæœ€æ–°5ç§’éŸ³é¢‘ è‡ªåŠ¨å†…å­˜ç®¡ç†é¿å…ç¼“å†²åŒºæº¢å‡º å®æ—¶æ€§èƒ½ä¼˜åŒ–ï¼š æ¨¡å‹é¢„åŠ è½½å‡å°‘æ¨ç†å»¶è¿Ÿ å¼‚æ­¥å¤„ç†æé«˜å“åº”é€Ÿåº¦ ç”¨æˆ·ä½“éªŒè®¾è®¡ï¼š å®æ—¶åé¦ˆæä¾›å³æ—¶æƒ…æ„Ÿè¯†åˆ«ç»“æœ å†å²è®°å½•åŠŸèƒ½æ”¯æŒç»“æœå›é¡¾ 4. å…³é”®å‚æ•°å«ä¹‰ä¸æ€§èƒ½å½±å“4.1 æ¨¡å‹ç»“æ„å‚æ•°æ·±åº¦åˆ†æ å‚æ•°å é»˜è®¤å€¼ å‚æ•°å«ä¹‰ æ€§èƒ½å½±å“æœºåˆ¶ è°ƒä¼˜å»ºè®® hidden_size 256 GRUéšçŠ¶æ€ç»´åº¦ è¡¨è¾¾èƒ½åŠ›ï¼šæ›´å¤§ç»´åº¦æä¾›æ›´å¼ºç‰¹å¾è¡¨è¾¾è®¡ç®—å¤æ‚åº¦ï¼šçº¿æ€§å½±å“å‚æ•°é‡å’Œè®¡ç®—æ—¶é—´è¿‡æ‹Ÿåˆé£é™©ï¼šè¿‡å¤§å¯èƒ½å¯¼è‡´è®­ç»ƒè¿‡æ‹Ÿåˆ 128-512èŒƒå›´å†…è°ƒä¼˜ç»“åˆdropouté˜²è¿‡æ‹Ÿåˆ dia_layers 2 GRUå †å å±‚æ•° æŠ½è±¡å±‚æ¬¡ï¼šå¤šå±‚æä¾›æ›´æ·±å±‚æ¬¡çš„ç‰¹å¾æŠ½è±¡æ¢¯åº¦ä¼ æ’­ï¼šè¿‡æ·±å¯èƒ½å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±è®­ç»ƒç¨³å®šæ€§ï¼šå±‚æ•°é€‚ä¸­ä¿è¯è®­ç»ƒç¨³å®š 1-4å±‚ä¸ºå®œé…åˆæ¢¯åº¦è£å‰ªä½¿ç”¨ utt_insize 768 HuBERTè¾“å‡ºç»´åº¦ ç‰¹å¾ä¸°å¯Œåº¦ï¼šå›ºå®šå€¼ï¼Œç”±é¢„è®­ç»ƒæ¨¡å‹å†³å®šåŒ¹é…è¦æ±‚ï¼šå¿…é¡»ä¸HuBERTè¾“å‡ºç»´åº¦ä¸€è‡´ ä¸å¯è°ƒæ•´ç”±é¢„è®­ç»ƒæ¨¡å‹å†³å®š out_class 4 æƒ…æ„Ÿç±»åˆ«æ•°é‡ ä»»åŠ¡å¤æ‚åº¦ï¼šç±»åˆ«æ•°ç›´æ¥å½±å“åˆ†ç±»éš¾åº¦æ•°æ®å¹³è¡¡ï¼šéœ€è¦å„ç±»åˆ«æ ·æœ¬ç›¸å¯¹å¹³è¡¡ æ ¹æ®å…·ä½“ä»»åŠ¡ç¡®å®šè€ƒè™‘ç±»åˆ«å¹³è¡¡ç­–ç•¥ 4.2 è®­ç»ƒè¶…å‚æ•°æ·±åº¦åˆ†æ å‚æ•°å é»˜è®¤å€¼ å‚æ•°å«ä¹‰ æ€§èƒ½å½±å“æœºåˆ¶ è°ƒä¼˜ç­–ç•¥ learning_rate 1e-5 å­¦ä¹ ç‡ æ”¶æ•›é€Ÿåº¦ï¼šè¿‡å¤§æ˜“éœ‡è¡ï¼Œè¿‡å°æ”¶æ•›æ…¢æœ€ç»ˆæ€§èƒ½ï¼šå½±å“æ¨¡å‹æ”¶æ•›åˆ°çš„å±€éƒ¨æœ€ä¼˜è§£è®­ç»ƒç¨³å®šæ€§ï¼šé€‚å½“å­¦ä¹ ç‡ä¿è¯è®­ç»ƒç¨³å®š 1e-6åˆ°1e-4èŒƒå›´ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦ dropout 0.2 éšæœºå¤±æ´»æ¦‚ç‡ æ­£åˆ™åŒ–å¼ºåº¦ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆçš„å…³é”®å‚æ•°æ¨¡å‹å®¹é‡ï¼šè¿‡å¤§å½±å“æ¨¡å‹è¡¨è¾¾èƒ½åŠ›æ³›åŒ–èƒ½åŠ›ï¼šé€‚å½“dropoutæå‡æ³›åŒ–æ€§èƒ½ 0.1-0.5èŒƒå›´è°ƒä¼˜æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´ batch_size 32 æ‰¹æ¬¡å¤§å° æ¢¯åº¦ä¼°è®¡ï¼šå½±å“æ¢¯åº¦ä¼°è®¡çš„å‡†ç¡®æ€§å†…å­˜å ç”¨ï¼šç›´æ¥å½±å“GPUå†…å­˜éœ€æ±‚è®­ç»ƒé€Ÿåº¦ï¼šå½±å“æ¯ä¸ªepochçš„è®­ç»ƒæ—¶é—´ 16-64æ ¹æ®æ˜¾å­˜è°ƒæ•´è€ƒè™‘æ¢¯åº¦ç´¯ç§¯ attention True æ³¨æ„åŠ›æœºåˆ¶å¼€å…³ é•¿åºåˆ—å»ºæ¨¡ï¼šæå‡é•¿è·ç¦»ä¾èµ–æ•è·èƒ½åŠ›è®¡ç®—å¼€é”€ï¼šå¢åŠ çº¦20%çš„è®¡ç®—æ—¶é—´æ¨¡å‹å¤æ‚åº¦ï¼šå¢åŠ æ¨¡å‹å‚æ•°é‡ æ ¹æ®åºåˆ—é•¿åº¦å†³å®šçŸ­åºåˆ—å¯å…³é—­ 4.3 æ•°æ®å¤„ç†å‚æ•°æ·±åº¦åˆ†æ å‚æ•°å é»˜è®¤å€¼ å‚æ•°å«ä¹‰ æ€§èƒ½å½±å“æœºåˆ¶ è®¾è®¡è€ƒé‡ time_seconds 3 éŸ³é¢‘å›ºå®šé•¿åº¦ ä¿¡æ¯å®Œæ•´æ€§ï¼šæ—¶é•¿å½±å“æƒ…æ„Ÿä¿¡æ¯çš„å®Œæ•´æ€§è®¡ç®—æ•ˆç‡ï¼šé•¿åº¦ç›´æ¥å½±å“è®¡ç®—å¤æ‚åº¦å†…å­˜å ç”¨ï¼šå½±å“æ‰¹å¤„ç†çš„å†…å­˜éœ€æ±‚ 2-5ç§’èŒƒå›´å†…å¹³è¡¡ä¿¡æ¯ä¸æ•ˆç‡ sample_rate 16000 éŸ³é¢‘é‡‡æ ·ç‡ é¢‘ç‡åˆ†è¾¨ç‡ï¼šå½±å“é«˜é¢‘ä¿¡æ¯çš„ä¿ç•™å…¼å®¹æ€§ï¼šéœ€åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹è¦æ±‚æ•°æ®å¤§å°ï¼šå½±å“éŸ³é¢‘æ•°æ®çš„å­˜å‚¨ç©ºé—´ å›ºå®š16kHzåŒ¹é…HuBERTè¦æ±‚ num_folds 5 äº¤å‰éªŒè¯æŠ˜æ•° è¯„ä¼°å¯é æ€§ï¼šæŠ˜æ•°è¶Šå¤šè¯„ä¼°è¶Šå¯é è®¡ç®—æˆæœ¬ï¼šæŠ˜æ•°å½±å“æ€»è®­ç»ƒæ—¶é—´ç»Ÿè®¡æ˜¾è‘—æ€§ï¼šå½±å“ç»“æœçš„ç»Ÿè®¡æ„ä¹‰ 5-10æŠ˜ä¸ºå®œå¹³è¡¡å¯é æ€§ä¸æˆæœ¬ 4.4 å‚æ•°è°ƒä¼˜çš„ç³»ç»Ÿæ€§æ–¹æ³•å±‚æ¬¡åŒ–è°ƒä¼˜ç­–ç•¥ï¼š æ¶æ„å‚æ•°ï¼šå…ˆç¡®å®šhidden_sizeå’Œdia_layers è®­ç»ƒå‚æ•°ï¼šå†è°ƒä¼˜learning_rateå’Œdropout æ•°æ®å‚æ•°ï¼šæœ€åä¼˜åŒ–batch_sizeå’Œtime_seconds æ€§èƒ½ç›‘æ§æŒ‡æ ‡ï¼š è®­ç»ƒæŒ‡æ ‡ï¼šæŸå¤±å‡½æ•°æ”¶æ•›æ›²çº¿ã€æ¢¯åº¦èŒƒæ•° éªŒè¯æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ã€F1åˆ†æ•°ã€æ··æ·†çŸ©é˜µ æ•ˆç‡æŒ‡æ ‡ï¼šè®­ç»ƒæ—¶é—´ã€å†…å­˜å ç”¨ã€æ¨ç†é€Ÿåº¦ 5. æ¨¡å‹å·¥ä½œæœºåˆ¶æ·±å…¥ç†è§£5.1 è‡ªç›‘ç£é¢„è®­ç»ƒçš„æ·±å±‚ä»·å€¼HuBERTæ¨¡å‹çš„é¢„è®­ç»ƒæœºåˆ¶ä½“ç°äº†ç°ä»£è¯­éŸ³å¤„ç†çš„æ ¸å¿ƒæ€æƒ³ï¼š æ©ç é¢„æµ‹ä»»åŠ¡çš„è®¾è®¡æ™ºæ…§ï¼š # HuBERTé¢„è®­ç»ƒä¼ªä»£ç ç¤ºä¾‹ masked_features = mask_features(input_features, mask_prob=0.15) predicted_features = hubert_model(masked_features) loss = mse_loss(predicted_features, target_features) å¤šå±‚æ¬¡ç‰¹å¾å­¦ä¹ æœºåˆ¶ï¼š å£°å­¦å±‚é¢ï¼šåº•å±‚Transformerå±‚å­¦ä¹ éŸ³ç´ ã€éŸ³è°ƒã€è¯­é€Ÿç­‰åŸºç¡€å£°å­¦ç‰¹å¾ è¯­è¨€å±‚é¢ï¼šä¸­å±‚å­¦ä¹ è¯æ±‡è¾¹ç•Œã€è¯­æ³•ç»“æ„ã€è¯­ä¹‰å…³ç³» éŸµå¾‹å±‚é¢ï¼šé«˜å±‚æ•è·èŠ‚å¥ã€é‡éŸ³ã€è¯­è°ƒå˜åŒ–ï¼Œè¿™äº›ç‰¹å¾ä¸æƒ…æ„Ÿè¡¨è¾¾å¯†åˆ‡ç›¸å…³ è¿ç§»å­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼š é¢„è®­ç»ƒç‰¹å¾åŒ…å«ä¸°å¯Œçš„è¯­éŸ³é€šç”¨è¡¨ç¤º åœ¨æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸Šå¾®è°ƒæ—¶ï¼Œæ¨¡å‹èƒ½å¿«é€Ÿé€‚åº”ç‰¹å®šé¢†åŸŸç‰¹å¾ ç›¸æ¯”ä»é›¶è®­ç»ƒï¼Œæ˜¾è‘—å‡å°‘äº†æ‰€éœ€çš„æ ‡æ³¨æ•°æ®é‡ 5.2 åºåˆ—å»ºæ¨¡çš„æ—¶åºä¾èµ–æœºåˆ¶åŒå‘GRUçš„é—¨æ§æœºåˆ¶å®ç°äº†å¯¹æ—¶åºä¿¡æ¯çš„ç²¾ç¡®æ§åˆ¶ï¼š é—¨æ§æœºåˆ¶çš„æ•°å­¦è¡¨è¾¾ï¼š # GRUé—¨æ§æœºåˆ¶ä¼ªä»£ç  reset_gate = sigmoid(W_r @ [h_prev, x_t]) update_gate = sigmoid(W_u @ [h_prev, x_t]) candidate_h = tanh(W_h @ [reset_gate * h_prev, x_t]) h_t = (1 - update_gate) * h_prev + update_gate * candidate_h åŒå‘ä¿¡æ¯èåˆçš„ä¼˜åŠ¿ï¼š å‰å‘æµï¼šæ•è·ä»è¯­éŸ³å¼€å§‹åˆ°å½“å‰ä½ç½®çš„æƒ…æ„Ÿå‘å±•è¶‹åŠ¿ åå‘æµï¼šåˆ©ç”¨æœªæ¥ä¿¡æ¯ä¸ºå½“å‰åˆ¤æ–­æä¾›ä¸Šä¸‹æ–‡çº¦æŸ ä¿¡æ¯äº’è¡¥ï¼šå‰åå‘ä¿¡æ¯çš„ç»“åˆæä¾›äº†æ›´å…¨é¢çš„æ—¶åºè¡¨ç¤º æƒ…æ„Ÿæ—¶åºæ¨¡å¼çš„å»ºæ¨¡ï¼š æƒ…æ„Ÿèµ·ä¼ï¼šGRUèƒ½å¤Ÿè®°å¿†æƒ…æ„Ÿçš„å˜åŒ–è½¨è¿¹ å…³é”®è½¬æŠ˜ï¼šé—¨æ§æœºåˆ¶è‡ªåŠ¨è¯†åˆ«æƒ…æ„Ÿè¡¨è¾¾çš„é‡è¦æ—¶åˆ» ä¸Šä¸‹æ–‡ä¾èµ–ï¼šåŒå‘è®¾è®¡ç¡®ä¿æ¯ä¸ªæ—¶åˆ»éƒ½èƒ½è·å¾—å……åˆ†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ 5.3 æ³¨æ„åŠ›æœºåˆ¶çš„åŠ¨æ€èšç„¦åŸç†æ³¨æ„åŠ›æœºåˆ¶å®ç°äº†å¯¹åºåˆ—ä¿¡æ¯çš„æ™ºèƒ½é€‰æ‹©ï¼š æ³¨æ„åŠ›æƒé‡çš„å­¦ä¹ æœºåˆ¶ï¼š # æ³¨æ„åŠ›æƒé‡è®¡ç®—çš„æ ¸å¿ƒé€»è¾‘ similarity_scores = query @ keys.T # è®¡ç®—ç›¸ä¼¼åº¦ attention_weights = softmax(similarity_scores) # å½’ä¸€åŒ–æƒé‡ attended_features = attention_weights @ values # åŠ æƒèšåˆ åŠ¨æ€èšç„¦çš„å®ç°åŸç†ï¼š æŸ¥è¯¢é©±åŠ¨ï¼šæ¯ä¸ªæ—¶é—´æ­¥ä½œä¸ºæŸ¥è¯¢ï¼ŒåŠ¨æ€å…³æ³¨æ•´ä¸ªåºåˆ— ç›¸ä¼¼åº¦åŒ¹é…ï¼šå­¦ä¹ åˆ°çš„å˜æ¢çŸ©é˜µæ•è·æŸ¥è¯¢ä¸é”®çš„åŒ¹é…æ¨¡å¼ è‡ªé€‚åº”æƒé‡ï¼šä¸åŒæƒ…æ„Ÿç±»åˆ«ä¸‹çš„æ³¨æ„åŠ›æ¨¡å¼è‡ªåŠ¨åˆ†åŒ– æƒ…æ„Ÿå…³é”®ä¿¡æ¯çš„è¯†åˆ«ï¼š éŸµå¾‹é‡ç‚¹ï¼šè‡ªåŠ¨å…³æ³¨è¯­è°ƒå˜åŒ–å‰§çƒˆçš„æ—¶é—´æ®µ è¯­ä¹‰å…³é”®è¯ï¼šèšç„¦äºå¸¦æœ‰å¼ºæƒ…æ„Ÿè‰²å½©çš„è¯æ±‡ åœé¡¿æ¨¡å¼ï¼šè¯†åˆ«æƒ…æ„Ÿè¡¨è¾¾ä¸­çš„åœé¡¿å’ŒèŠ‚å¥å˜åŒ– 5.4 å…¨å±€æ± åŒ–çš„ä¿¡æ¯èšåˆç­–ç•¥æœ€å¤§æ± åŒ–æ“ä½œå®ç°äº†ä»åºåˆ—åˆ°å…¨å±€ç‰¹å¾çš„è½¬æ¢ï¼š æœ€å¤§æ± åŒ–çš„é€‰æ‹©rationaleï¼š # æœ€å¤§æ± åŒ– vs å¹³å‡æ± åŒ–çš„å¯¹æ¯” max_pooled = F.max_pool1d(features, kernel_size=seq_len) # ä¿ç•™æœ€å¼ºä¿¡å· avg_pooled = F.avg_pool1d(features, kernel_size=seq_len) # å¹³å‡æ‰€æœ‰ä¿¡å· æƒ…æ„Ÿè¯†åˆ«ä¸­çš„ä¼˜åŠ¿ï¼š æ˜¾è‘—æ€§ä¿ç•™ï¼šæœ€å¤§æ± åŒ–ä¿ç•™æœ€å¼ºçš„æƒ…æ„Ÿæ¿€æ´»ä¿¡å· å™ªå£°æŠ‘åˆ¶ï¼šå¿½ç•¥å¼±æ¿€æ´»çš„å™ªå£°ä¿¡æ¯ ä¸å˜æ€§ï¼šå¯¹åºåˆ—é•¿åº¦å˜åŒ–å…·æœ‰ä¸€å®šçš„é²æ£’æ€§ 6. ç³»ç»Ÿä¼˜åŠ¿ä¸æŠ€æœ¯åˆ›æ–°6.1 ç«¯åˆ°ç«¯å­¦ä¹ èŒƒå¼çš„æŠ€æœ¯çªç ´ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ï¼š æ‰‹å·¥ç‰¹å¾è®¾è®¡ä¾èµ–é¢†åŸŸä¸“å®¶çŸ¥è¯† ç‰¹å¾æå–ä¸åˆ†ç±»å™¨åˆ†ç¦»è®­ç»ƒï¼Œæ— æ³•å®ç°å…¨å±€ä¼˜åŒ– ç‰¹å¾è¡¨è¾¾èƒ½åŠ›å—é™äºäººå·¥è®¾è®¡çš„æƒ³è±¡åŠ› ç«¯åˆ°ç«¯å­¦ä¹ çš„ä¼˜åŠ¿ï¼š è‡ªåŠ¨ç‰¹å¾å­¦ä¹ ï¼šæ¨¡å‹è‡ªåŠ¨å‘ç°æœ€ä¼˜çš„ç‰¹å¾è¡¨ç¤º å…¨å±€ä¼˜åŒ–ï¼šä»åŸå§‹è¾“å…¥åˆ°æœ€ç»ˆè¾“å‡ºçš„è”åˆä¼˜åŒ– é€‚åº”æ€§å¼ºï¼šèƒ½å¤Ÿé€‚åº”ä¸åŒçš„æ•°æ®åˆ†å¸ƒå’Œä»»åŠ¡éœ€æ±‚ 6.2 å¤šå±‚æ¬¡ç‰¹å¾èåˆçš„åˆ›æ–°è®¾è®¡ç‰¹å¾èåˆçš„å±‚æ¬¡ç»“æ„ï¼š HuBERTç‰¹å¾(768ç»´) â†’ GRUæ—¶åºå»ºæ¨¡(512ç»´) â†’ æ³¨æ„åŠ›å¢å¼º â†’ å…¨å±€æ± åŒ– â†’ åˆ†ç±»è¾“å‡º èåˆæœºåˆ¶çš„æŠ€æœ¯åˆ›æ–°ï¼š è¯­ä¹‰-æ—¶åºèåˆï¼šHuBERTçš„è¯­ä¹‰ç‰¹å¾ä¸GRUçš„æ—¶åºå»ºæ¨¡ç›¸ç»“åˆ å±€éƒ¨-å…¨å±€èåˆï¼šæ³¨æ„åŠ›æœºåˆ¶å®ç°å±€éƒ¨ç‰¹å¾ä¸å…¨å±€ä¸Šä¸‹æ–‡çš„èåˆ é™æ€-åŠ¨æ€èåˆï¼šé™æ€çš„é¢„è®­ç»ƒç‰¹å¾ä¸åŠ¨æ€çš„åºåˆ—å»ºæ¨¡ç›¸ç»“åˆ 6.3 æ³¨æ„åŠ›å¢å¼ºæœºåˆ¶çš„åŸåˆ›æ€§åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä¸­çš„åˆ›æ–°åº”ç”¨ï¼š æ—¶åºæ³¨æ„åŠ›ï¼šé’ˆå¯¹è¯­éŸ³çš„æ—¶åºç‰¹æ€§è®¾è®¡çš„æ³¨æ„åŠ›æœºåˆ¶ æƒ…æ„Ÿèšç„¦ï¼šè‡ªåŠ¨è¯†åˆ«æƒ…æ„Ÿè¡¨è¾¾çš„å…³é”®æ—¶é—´æ®µ å¯è§£é‡Šæ€§ï¼šæ³¨æ„åŠ›æƒé‡æä¾›æ¨¡å‹å†³ç­–çš„å¯è§†åŒ–è§£é‡Š 6.4 å·¥ç¨‹åŒ–éƒ¨ç½²çš„å…¨é¢è€ƒè™‘ç³»ç»Ÿå·¥ç¨‹åŒ–çš„å®Œæ•´æ€§ï¼š æ¨¡å—åŒ–è®¾è®¡ï¼šæ¸…æ™°çš„ä»£ç ç»“æ„ä¾¿äºç»´æŠ¤å’Œæ‰©å±• å®æ—¶å¤„ç†èƒ½åŠ›ï¼šæ”¯æŒéº¦å…‹é£å®æ—¶å½•éŸ³å’Œæƒ…æ„Ÿè¯†åˆ« ç”¨æˆ·å‹å¥½ç•Œé¢ï¼šå®Œæ•´çš„PyQt5å›¾å½¢ç•Œé¢ è·¨å¹³å°å…¼å®¹ï¼šæ”¯æŒä¸åŒæ“ä½œç³»ç»Ÿçš„éƒ¨ç½² éƒ¨ç½²ä¼˜åŒ–çš„æŠ€æœ¯ç»†èŠ‚ï¼š æ¨¡å‹å‹ç¼©ï¼šé€šè¿‡é‡åŒ–ç­‰æŠ€æœ¯å‡å°‘æ¨¡å‹å¤§å° æ¨ç†åŠ é€Ÿï¼šGPUåŠ é€Ÿå’Œæ‰¹å¤„ç†ä¼˜åŒ– å†…å­˜ç®¡ç†ï¼šé«˜æ•ˆçš„éŸ³é¢‘ç¼“å†²å’Œç‰¹å¾ç¼“å­˜æœºåˆ¶ 6.5 è¯„ä¼°æ–¹æ³•çš„ç§‘å­¦æ€§5æŠ˜äº¤å‰éªŒè¯çš„ç»Ÿè®¡ä¸¥è°¨æ€§ï¼š ç¡®ä¿ç»“æœçš„ç»Ÿè®¡æ˜¾è‘—æ€§å’Œå¯é‡ç°æ€§ é¿å…æ•°æ®åˆ’åˆ†å¶ç„¶æ€§å¯¹ç»“æœçš„å½±å“ æä¾›æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„å¯é ä¼°è®¡ å¤šæŒ‡æ ‡è¯„ä¼°çš„å…¨é¢æ€§ï¼š å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°çš„ç»¼åˆè¯„ä¼° æ··æ·†çŸ©é˜µåˆ†æå„ç±»åˆ«çš„è¯†åˆ«æ€§èƒ½ ç»Ÿè®¡æ£€éªŒç¡®ä¿ç»“æœçš„ç§‘å­¦æ€§ æ€»ç»“è¯¥IEMOCAPè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿå±•ç°äº†ç°ä»£æ·±åº¦å­¦ä¹ åœ¨è¯­éŸ³å¤„ç†é¢†åŸŸçš„å…ˆè¿›æŠ€æœ¯åº”ç”¨ã€‚é€šè¿‡HuBERTé¢„è®­ç»ƒæ¨¡å‹çš„å¼ºå¤§ç‰¹å¾æå–èƒ½åŠ›ã€åŒå‘GRUçš„ç²¾ç¡®æ—¶åºå»ºæ¨¡ã€æ³¨æ„åŠ›æœºåˆ¶çš„æ™ºèƒ½èšç„¦ï¼Œä»¥åŠå…¨å±€æ± åŒ–çš„æœ‰æ•ˆä¿¡æ¯èšåˆï¼Œç³»ç»Ÿå®ç°äº†ä»åŸå§‹éŸ³é¢‘åˆ°æƒ…æ„Ÿç±»åˆ«çš„ç«¯åˆ°ç«¯å­¦ä¹ ã€‚ ç³»ç»Ÿçš„æŠ€æœ¯åˆ›æ–°ä½“ç°åœ¨å¤šä¸ªæ–¹é¢ï¼šè‡ªç›‘ç£é¢„è®­ç»ƒä¸ä¸‹æ¸¸ä»»åŠ¡çš„æœ‰æ•ˆç»“åˆã€å¤šå±‚æ¬¡ç‰¹å¾çš„æ·±åº¦èåˆã€æ³¨æ„åŠ›æœºåˆ¶çš„åŸåˆ›æ€§åº”ç”¨ï¼Œä»¥åŠå·¥ç¨‹åŒ–éƒ¨ç½²çš„å…¨é¢è€ƒè™‘ã€‚è¿™äº›è®¾è®¡ä¸ä»…ä¿è¯äº†æ¨¡å‹çš„é«˜æ€§èƒ½ï¼Œä¹Ÿä¸ºå®é™…åº”ç”¨æä¾›äº†åšå®çš„æŠ€æœ¯åŸºç¡€ã€‚ é€šè¿‡æ·±å…¥çš„æºç åˆ†æï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¯¥ç³»ç»Ÿä¸ä»…æ˜¯ä¸€ä¸ªæŠ€æœ¯å®ç°ï¼Œæ›´æ˜¯å¯¹è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«é¢†åŸŸå‰æ²¿æŠ€æœ¯çš„ç³»ç»Ÿæ€§æ•´åˆå’Œåˆ›æ–°æ€§åº”ç”¨ã€‚å®ƒä¸ºç›¸å…³ç ”ç©¶å’Œåº”ç”¨å¼€å‘æä¾›äº†å®è´µçš„å‚è€ƒå’Œå€Ÿé‰´ä»·å€¼ã€‚","link":"/2025/10/01/%E4%BB%A3%E7%A0%81%E4%BC%98%E5%8C%96%E8%AF%A6%E7%BB%86%E6%96%87%E6%A1%A3/"}],"tags":[{"name":"yoloæ¨¡å‹","slug":"yoloæ¨¡å‹","link":"/tags/yolo%E6%A8%A1%E5%9E%8B/"},{"name":"me","slug":"me","link":"/tags/me/"},{"name":"GRUç»“æ„","slug":"GRUç»“æ„","link":"/tags/GRU%E7%BB%93%E6%9E%84/"}],"categories":[{"name":"è®¡ç®—æœºè§†è§‰","slug":"è®¡ç®—æœºè§†è§‰","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"è‡ªç„¶è¯­è¨€å¤„ç†","slug":"è‡ªç„¶è¯­è¨€å¤„ç†","link":"/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"}],"pages":[]}